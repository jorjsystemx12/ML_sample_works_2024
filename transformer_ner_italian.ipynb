{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f56ba4",
   "metadata": {},
   "source": [
    "## Italian-Language BERT Tranformer Model Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84536e6b",
   "metadata": {},
   "source": [
    "<b> Italian NER variant: Fine-tune a pretrained BERT model on the Huggingface polyglot_ner data to perform named entity recognition. The data consists of multiple languages. For this task, only one language (Italian) will be selected to fine-tune the BERT model. The fine-tuning will performed 3 times. Once with a dataset composed of 1000 sentences, a second with 3000 sentences and lastly a third with 3000 sentences and frozen embedding</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "685032a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the main tools for the task, copied from previous exercise\n",
    "## Done on just in case basis\n",
    "\n",
    "# Import the necessary packages and libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf8622",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running the basic installation:\n",
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "136da90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we experiment with datasets:\n",
    "## Main instruments needed to retrieve Huggingface datasets in convenient manner:\n",
    "# Source: https://huggingface.co/docs/datasets/load_hub\n",
    "# Source: https://huggingface.co/docs/datasets/index\n",
    "from datasets import load_dataset\n",
    "\n",
    "#Ignore filterwarnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# Print all the available datasets\n",
    "from huggingface_hub import list_datasets\n",
    "\n",
    "#Check all the datasets available:\n",
    "#print([dataset.id for dataset in list_datasets()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba29e08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Check the data type:\n",
    "#print(type([dataset.id for dataset in list_datasets()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0668b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polyglot_ner is here!\n"
     ]
    }
   ],
   "source": [
    "## Interested in the polyglot-ner dataset (https://huggingface.co/datasets/polyglot_ner)\n",
    "# Find it by matching directly:\n",
    "datasetlist = [dataset.id for dataset in list_datasets()]\n",
    "for elem in datasetlist:\n",
    "    if elem == 'polyglot_ner':\n",
    "        print('polyglot_ner is here!')\n",
    "        \n",
    "## Yes, polyglot-ner dataset is available:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1750e893",
   "metadata": {},
   "source": [
    "<b> We want to fine-tune the BERT model on one of the languages (Italian) of the dataset that fulfill the following requirements:  (1) is not English, (2) Has already a pretrained BERT-base. (3) The language contains at least 7k sentences. The following code block will attempt to find a language with these conditions. </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e388aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "## First we try with streaming to avoid downloading the massive set:\n",
    "## We also check if we can isolate the language of interest:\n",
    "polyglot_ner_mainset = load_dataset('polyglot_ner', 'it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08121f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'lang', 'words', 'ner'],\n",
      "        num_rows: 378325\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "## Check attributes and data:\n",
    "print(polyglot_ner_mainset)\n",
    "\n",
    "## Italian has around 378K sentences and the results seems to match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9494ee25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517057821e8a4bacb3a2540d89f6f53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/378325 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Save it:\n",
    "polyglot_ner_mainset.save_to_disk(\"it.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecd3fef",
   "metadata": {},
   "source": [
    "<b> The \"it\" for Italian language, with 378325 sentences which meets and exceeds the requirements, will be used to fine-tune the pretrained BERT-model (Source: https://huggingface.co/dbmdz/bert-base-italian-cased). Next the data will be prepared</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4d55a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "## Now we check the mainset in Italian:\n",
    "# First element/sentence returned a success!\n",
    "print(polyglot_ner_mainset['train']['ner'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11c4b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the BertForTokenClassification, Tokenizer for BERT and pipeline system:\n",
    "# Bring back the dataloader and dataset builder as done in the previous CNN and RNN assignments:\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# tqdm needed to see measure bars\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import pipeline\n",
    "from transformers import BertForTokenClassification, BertTokenizer, DataCollatorForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7c32c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Italian tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('dbmdz/bert-base-italian-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f237ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and align labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # Truncation is necessary and padding is applied to the various sentence lengths\n",
    "    tokenized_inputs = tokenizer(examples['words'], truncation=True, padding='max_length', is_split_into_words=True, max_length=128)\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # Use label dictionary to convert string labels to integer\n",
    "                label_ids.append(label_to_id[label[word_idx]])\n",
    "            else:\n",
    "                # For subwords/wordpieces, set label to -100 (ignored in loss step)\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89babd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label_to_id dictionary for index creation:\n",
    "label_to_id = {label: i for i, label in enumerate(set([lbl for sublist in polyglot_ner_mainset['train']['ner'] for lbl in sublist]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "782ef6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': 0, 'ORG': 1, 'O': 2, 'PER': 3}\n"
     ]
    }
   ],
   "source": [
    "## Check the dictionary of 4 elems: location being 0, organization 1, 'O' at 2 and person 3:\n",
    "print(label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5201fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde80f08d8d84e6fbd32ad2751ee27e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/378325 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the function to tokenize and align labels\n",
    "tokenized_it_ner_dataset = polyglot_ner_mainset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48854395",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e559d614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'lang': 'it', 'words': ['Ma', 'tra', 'il', 'prigioniero', 'e', 'la', 'sua', 'carceriera', 'nasce', 'un', 'rapporto', 'malato', 'basato', 'su', 'violenza', 'e', 'amore', ',', 'passione', 'e', 'tortura', 'per', 'un', 'lieto', 'fine', 'azzardato', 'e', 'difficile', 'da', 'digerire', '.'], 'ner': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'input_ids': [102, 348, 293, 162, 15297, 126, 146, 497, 646, 10631, 218, 7514, 141, 2899, 12338, 5973, 171, 6632, 126, 3711, 1307, 8164, 126, 11760, 156, 141, 12809, 1027, 10201, 863, 112, 126, 2726, 203, 120, 28184, 113, 697, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, 2, 2, 2, 2, -100, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    }
   ],
   "source": [
    "# Check the full tokenized dataset that has been padded, tokenized, truncated\n",
    "## The output looks good, dataset is ready for lock and loading:\n",
    "print(tokenized_it_ner_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f3e707",
   "metadata": {},
   "source": [
    "<b> Now that the data is tokenized and processed, we will load the pretrained model, define a trainer function as suggested by the instructions.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a3529a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import all the needed instruments for the training, BERT configurations for tokens and arguments:\n",
    "from transformers import BertForTokenClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('dbmdz/bert-base-italian-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a926b5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model with a token classification head\n",
    "num_labels = len(label_to_id)\n",
    "model = BertForTokenClassification.from_pretrained('dbmdz/bert-base-italian-cased', num_labels=num_labels)\n",
    "\n",
    "### Special Warning provided ### Analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da2b9439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish training args:\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf3a107",
   "metadata": {},
   "source": [
    "<b> First we will fine-tune the model using the first dataset made of only 1000 sentences of the Italian subset.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3899cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the metric instruments from SKLearn:\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Create a subset of the dataset for training (first 1000 sentences)\n",
    "train_subset_1k = tokenized_it_ner_dataset['train'].select(range(1000))\n",
    "\n",
    "# Create a subset for testing (next 200 sentences)\n",
    "test_subset = tokenized_it_ner_dataset['train'].select(range(1000, 1200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "869feead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for computing metrics:\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # Flatten the lists and exclude labels for special tokens (i.e., -100)\n",
    "    flat_labels = [label for sublist in labels for label in sublist if label != -100]\n",
    "    flat_preds = [pred for sublist, label_sublist in zip(preds, labels) for pred, label in zip(sublist, label_sublist) if label != -100]\n",
    "\n",
    "    accuracy = accuracy_score(flat_labels, flat_preds)\n",
    "    f1_micro = f1_score(flat_labels, flat_preds, average='micro')\n",
    "    f1_macro = f1_score(flat_labels, flat_preds, average='macro')\n",
    "\n",
    "    ## Returns multiple key metrics of interest:\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24e06358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer with the training subset, test subset, and compute_metrics function\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset_1k,\n",
    "    eval_dataset=test_subset,  \n",
    "    compute_metrics=compute_metrics  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad826594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 19:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.090421</td>\n",
       "      <td>0.961910</td>\n",
       "      <td>0.961910</td>\n",
       "      <td>0.761140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.052800</td>\n",
       "      <td>0.083081</td>\n",
       "      <td>0.968599</td>\n",
       "      <td>0.968599</td>\n",
       "      <td>0.760925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.087292</td>\n",
       "      <td>0.966741</td>\n",
       "      <td>0.966741</td>\n",
       "      <td>0.757627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=0.06296443198093031, metrics={'train_runtime': 1171.8441, 'train_samples_per_second': 2.56, 'train_steps_per_second': 0.161, 'total_flos': 195976111104000.0, 'train_loss': 0.06296443198093031, 'epoch': 3.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with 3 epochs:\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d7830b",
   "metadata": {},
   "source": [
    "<b>TrainOutput(global_step=189, training_loss=0.16357500180996284, metrics={'train_runtime': 1523.0356, 'train_samples_per_second': 1.97, 'train_steps_per_second': 0.124, 'total_flos': 195976111104000.0, 'train_loss': 0.16357500180996284, 'epoch': 3.0}) <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "098efaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08729159086942673, 'eval_accuracy': 0.9667409884801189, 'eval_f1_micro': 0.9667409884801189, 'eval_f1_macro': 0.7576269915990677, 'eval_runtime': 26.0691, 'eval_samples_per_second': 7.672, 'eval_steps_per_second': 0.153, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and save the model:\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./italian_bert_model_1k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650f22a4",
   "metadata": {},
   "source": [
    "<b>{'eval_loss': 0.08729159086942673, 'eval_accuracy': 0.9667409884801189, 'eval_f1_micro': 0.9667409884801189, 'eval_f1_macro': 0.7576269915990677, 'eval_runtime': 26.0691, 'eval_samples_per_second': 7.672, 'eval_steps_per_second': 0.153, 'epoch': 3.0}<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7b345",
   "metadata": {},
   "source": [
    "<b> Next, train the model with 3000 samples. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0609db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the dataset for training (first 3000 sentences)\n",
    "train_subset_3k = tokenized_it_ner_dataset['train'].select(range(3000))\n",
    "\n",
    "# Create a subset for testing (next 200 sentences)\n",
    "test_subset_3k = tokenized_it_ner_dataset['train'].select(range(3000, 3200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b37ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer with the training subset, test subset, and compute_metrics function\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset_3k,\n",
    "    eval_dataset=test_subset_3k,  \n",
    "    compute_metrics=compute_metrics  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8527a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 57:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.075500</td>\n",
       "      <td>0.095812</td>\n",
       "      <td>0.955146</td>\n",
       "      <td>0.955146</td>\n",
       "      <td>0.673774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.094063</td>\n",
       "      <td>0.956449</td>\n",
       "      <td>0.956449</td>\n",
       "      <td>0.691412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.096860</td>\n",
       "      <td>0.955332</td>\n",
       "      <td>0.955332</td>\n",
       "      <td>0.686150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=564, training_loss=0.06880511898309627, metrics={'train_runtime': 3434.8856, 'train_samples_per_second': 2.62, 'train_steps_per_second': 0.164, 'total_flos': 587928333312000.0, 'train_loss': 0.06880511898309627, 'epoch': 3.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train again with just 3 epochs:\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2028b9",
   "metadata": {},
   "source": [
    "<b> Results: TrainOutput(global_step=564, training_loss=0.06880511898309627, metrics={'train_runtime': 3434.8856, 'train_samples_per_second': 2.62, 'train_steps_per_second': 0.164, 'total_flos': 587928333312000.0, 'train_loss': 0.06880511898309627, 'epoch': 3.0}) <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "859e897b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09686033427715302, 'eval_accuracy': 0.9553322166387493, 'eval_f1_micro': 0.9553322166387493, 'eval_f1_macro': 0.6861496257407507, 'eval_runtime': 27.6687, 'eval_samples_per_second': 7.228, 'eval_steps_per_second': 0.145, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./italian_bert_model_3k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7333e1d",
   "metadata": {},
   "source": [
    "<b>{'eval_loss': 0.09686033427715302, 'eval_accuracy': 0.9553322166387493, 'eval_f1_micro': 0.9553322166387493, 'eval_f1_macro': 0.6861496257407507, 'eval_runtime': 27.6687, 'eval_samples_per_second': 7.228, 'eval_steps_per_second': 0.145, 'epoch': 3.0}<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db56ce",
   "metadata": {},
   "source": [
    "<b> Lastly, build a model with 3000 sentences again but with frozen embedding = embedding weights of the pretrained model are retained as they are \"frozen\" but the other weights of the model will be changed. Useful when having small dataset and to avoid overfitting issues.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d164f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('dbmdz/bert-base-italian-cased')\n",
    "\n",
    "# Load the pretrained model \n",
    "num_labels = len(label_to_id)\n",
    "model = BertForTokenClassification.from_pretrained('dbmdz/bert-base-italian-cased', num_labels=num_labels)\n",
    "\n",
    "# Freeze the embeddings. \n",
    "# Source: https://discuss.huggingface.co/t/how-to-freeze-some-layers-of-bertmodel/917\n",
    "for param in model.bert.embeddings.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "680ffa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish args for the model training:\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "671697ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the dataset for training of 3000 units\n",
    "train_subset_3k = tokenized_it_ner_dataset['train'].select(range(3000, 6000))\n",
    "# Create testing kits (next 200 sentences)\n",
    "test_subset = tokenized_it_ner_dataset['train'].select(range(6000, 6200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6de367d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final trainer for the 3k with frozen embeds:\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset_3k,\n",
    "    eval_dataset=test_subset,\n",
    "    compute_metrics=compute_metrics  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "80bcb65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 56:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.128500</td>\n",
       "      <td>0.076202</td>\n",
       "      <td>0.968985</td>\n",
       "      <td>0.968985</td>\n",
       "      <td>0.687896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>0.078107</td>\n",
       "      <td>0.965221</td>\n",
       "      <td>0.965221</td>\n",
       "      <td>0.693348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.078969</td>\n",
       "      <td>0.966475</td>\n",
       "      <td>0.966475</td>\n",
       "      <td>0.740800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=564, training_loss=0.09692417655853515, metrics={'train_runtime': 3392.3096, 'train_samples_per_second': 2.653, 'train_steps_per_second': 0.166, 'total_flos': 587928333312000.0, 'train_loss': 0.09692417655853515, 'epoch': 3.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train final model:\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f1b80",
   "metadata": {},
   "source": [
    "<b> TrainOutput(global_step=564, training_loss=0.09692417655853515, metrics={'train_runtime': 3392.3096, 'train_samples_per_second': 2.653, 'train_steps_per_second': 0.166, 'total_flos': 587928333312000.0, 'train_loss': 0.09692417655853515, 'epoch': 3.0})<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b904195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07896889001131058, 'eval_accuracy': 0.9664754392255288, 'eval_f1_micro': 0.9664754392255288, 'eval_f1_macro': 0.7407998544314155, 'eval_runtime': 25.676, 'eval_samples_per_second': 7.789, 'eval_steps_per_second': 0.156, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the last model like the others:\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "\n",
    "# Save the final 3k with freezing:\n",
    "model.save_pretrained('./italian_bert_model_3k_plus_frozen_embeds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6635fd6",
   "metadata": {},
   "source": [
    "<b>{'eval_loss': 0.07896889001131058, 'eval_accuracy': 0.9664754392255288, 'eval_f1_micro': 0.9664754392255288, 'eval_f1_macro': 0.7407998544314155, 'eval_runtime': 25.676, 'eval_samples_per_second': 7.789, 'eval_steps_per_second': 0.156, 'epoch': 3.0}<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850d2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FINISHED ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
