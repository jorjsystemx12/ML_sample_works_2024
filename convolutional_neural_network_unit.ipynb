{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e872c2",
   "metadata": {},
   "source": [
    "\n",
    "## Convolutional Neural Network Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb7c78",
   "metadata": {},
   "source": [
    "<b> Step one: Load the data and turn it into pandas. We have the labels and the text in different files so we will combine it into a pandas file. Dataset: https://github.com/cardiffnlp/tweeteval</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a26505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the main tools for the task:\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae1f3f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label_text  label_number\n",
      "0  â€œWorry is a down payment on a problem you may ...   optimism             2\n",
      "1  My roommate: it's okay that we can't spell bec...    sadness             0\n",
      "2  No but that's so cute. Atsu was probably shy a...        joy             1\n",
      "3  Rooneys fucking untouchable isn't he? Been fuc...    sadness             0\n",
      "4  it's pretty depressing when u hit pan on ur fa...      anger             3\n",
      "Length of train_data: 3257\n",
      "Length of validation_data: 374\n",
      "Length of test_data: 1421\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary of the mapping between numbers and labels\n",
    "mappings = {\"anger\": 3, \"joy\": 1, \"optimism\": 2, \"sadness\": 0}\n",
    "\n",
    "def load_data(mapping_dictionary:dict , tweet_file_path: str, label_file_path:str)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    function to load both the tweets and the labels, combine them together as pandas dataframe\n",
    "    \"\"\"\n",
    "    with open(tweet_file_path, 'r', encoding=\"utf-8\") as file:\n",
    "        tweets = [line.strip() for line in file.readlines()]\n",
    "\n",
    "    with open(label_file_path, 'r', encoding=\"utf-8\") as file:\n",
    "        label_numbers = [int(line.strip()) for line in file.readlines()]\n",
    "\n",
    "    label_texts = [next((key for key, value in mapping_dictionary.items() if value == label_number), None) for label_number in label_numbers]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'text': tweets,\n",
    "        'label_text': label_texts,\n",
    "        'label_number': label_numbers\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# make sure the files in the same directory as the notebook\n",
    "train_data = load_data(mappings, \"train_text.txt\", \"train_labels.txt\")\n",
    "validation_data = load_data(mappings, \"val_text.txt\", \"val_labels.txt\")\n",
    "test_data = load_data(mappings, \"test_text.txt\", \"test_labels.txt\")\n",
    "\n",
    "# print the head of one of them\n",
    "print(train_data.head())\n",
    "\n",
    "# Print the length of each data\n",
    "print(f\"Length of train_data: {len(train_data)}\")\n",
    "print(f\"Length of validation_data: {len(validation_data)}\")\n",
    "print(f\"Length of test_data: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0883a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label_text  label_number\n",
      "1  My roommate: it's okay that we can't spell bec...    sadness             0\n",
      "2  No but that's so cute. Atsu was probably shy a...        joy             1\n",
      "3  Rooneys fucking untouchable isn't he? Been fuc...    sadness             0\n",
      "5  @user but your pussy was weak from what I hear...    sadness             0\n",
      "7  Tiller and breezy should do a collab album. Ra...        joy             1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "2108\n"
     ]
    }
   ],
   "source": [
    "### Now for the main assignment:\n",
    "## Task 2 with a dataset from only 2 emotions\n",
    "# First, we grab the main dataset again, complete with training and validation packs:\n",
    "## Now we filter and create the subset dataset:\n",
    "# First one will be sadness (0) and joy (1)\n",
    "sadjoylist = ['sadness','joy']\n",
    "train_data = train_data[train_data['label_text'].isin(sadjoylist)]\n",
    "\n",
    "#Alright, got only the sadness and joy!\n",
    "print(train_data.head())\n",
    "print(type(train_data))\n",
    "## Count total with only sadness and joy:\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96da3ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we do the same for the other 2 sets for validation and testing:\n",
    "validation_data = validation_data[validation_data['label_text'].isin(sadjoylist)]\n",
    "test_data = test_data[test_data['label_text'].isin(sadjoylist)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12e5804d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label_text  label_number\n",
      "0  @user @user Oh, hidden revenge and anger...I r...    sadness             0\n",
      "1  if not then #teamchristine bc all tana has don...    sadness             0\n",
      "2  Hey @user #Fields in #skibbereen give your onl...    sadness             0\n",
      "3  Why have #Emmerdale had to rob #robron of havi...    sadness             0\n",
      "4  @user I would like to hear a podcast of you go...    sadness             0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "257\n"
     ]
    }
   ],
   "source": [
    "#Validation set got only the sadness and joy!\n",
    "print(validation_data.head())\n",
    "print(type(validation_data))\n",
    "# Count validation:\n",
    "print(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dcae2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label_text  label_number\n",
      "1  @user Interesting choice of words... Are you c...    sadness             0\n",
      "3  @user Welcome to #MPSVT! We are delighted to h...        joy             1\n",
      "4                       What makes you feel #joyful?        joy             1\n",
      "5                                    i am revolting.    sadness             0\n",
      "9  @user Get Donovan out of your soccer booth. He...    sadness             0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "916\n"
     ]
    }
   ],
   "source": [
    "#Test set got only the sadness and joy!\n",
    "print(test_data.head())\n",
    "print(type(test_data))\n",
    "#count:\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ad38cc",
   "metadata": {},
   "source": [
    "<b> Next step after loading the data is preprocessing, including tokenization, creating vocabulary, embedding and padding. We used this tutorial on how to build CNN text classifier https://chriskhanhtran.github.io/posts/cnn-sentence-classification/ </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4617a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize the strings in text column:\n",
    "def tokenize_sentence(sentence: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenizes a sentence using nltk's word_tokenize method.\n",
    "\n",
    "    Args:\n",
    "    - sentence (str): The sentence to tokenize.\n",
    "\n",
    "    Returns:\n",
    "    - List of tokens.\n",
    "    \"\"\"\n",
    "    return word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac8a4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we apply the tokenizer to the new subset and the validation/testing kit:\n",
    "train_data['tokenized_text'] = train_data['text'].apply(tokenize_sentence)\n",
    "validation_data['tokenized_text'] = validation_data['text'].apply(tokenize_sentence)\n",
    "test_data['tokenized_text'] = test_data['text'].apply(tokenize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca829343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    [My, roommate, :, it, 's, okay, that, we, ca, ...\n",
      "2    [No, but, that, 's, so, cute, ., Atsu, was, pr...\n",
      "3    [Rooneys, fucking, untouchable, is, n't, he, ?...\n",
      "5    [@, user, but, your, pussy, was, weak, from, w...\n",
      "7    [Tiller, and, breezy, should, do, a, collab, a...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# now the data has a new column \"tokenized_text\" which is a list of tokens\n",
    "#print(train_data.head())\n",
    "print(train_data['tokenized_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2570d02b",
   "metadata": {},
   "source": [
    "<b> After tokenization, we'll get set of all the unique tokens in the data and create a mapping between the tokens and their index. Then convert the tokens into numbers </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3895d1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['physical', 'inferno', 'Point', 'yuk']\n"
     ]
    }
   ],
   "source": [
    "# Build a set of all unique tokens in the training data\n",
    "vocab_set = set()\n",
    "for tokens in train_data['tokenized_text']:\n",
    "    vocab_set.update(tokens)\n",
    "\n",
    "# Convert the set to a list to index tokens\n",
    "vocab_list = list(vocab_set)\n",
    "\n",
    "print(vocab_list[:4])\n",
    "\n",
    "# Create a word to index mapping\n",
    "word_to_index = {word: index for index, word in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c28a8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add OOV token and its index to the vocabulary. This is because some tokens in the val/test might have vocabulary not in training\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "if OOV_TOKEN not in word_to_index:\n",
    "    word_to_index[OOV_TOKEN] = len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f4b27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main function for conversion of tokens from previous step to index numbers:\n",
    "def tokens_to_numbers(tokens: list, word_to_index: dict) -> list:\n",
    "    \"\"\"\n",
    "    Converts a list of tokens to their corresponding indices using a word-to-index mapping.\n",
    "    Returns the index of OOV_TOKEN for out-of-vocabulary words.\n",
    "    \"\"\"\n",
    "    return [word_to_index.get(token, word_to_index[OOV_TOKEN]) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebd3a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the function for token to index conversion:\n",
    "train_data['numeric_tokens'] = train_data['tokenized_text'].apply(lambda x: tokens_to_numbers(x, word_to_index))\n",
    "validation_data['numeric_tokens'] = validation_data['tokenized_text'].apply(lambda x: tokens_to_numbers(x, word_to_index))\n",
    "test_data['numeric_tokens'] = test_data['tokenized_text'].apply(lambda x: tokens_to_numbers(x, word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa5d93f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1       [1269, 2851, 2118, 7359, 2559, 5508, 1162, 594...\n",
      "2       [1212, 4011, 1162, 2559, 7581, 4218, 4141, 688...\n",
      "3       [3434, 3943, 6005, 5952, 3404, 1211, 4483, 705...\n",
      "5       [6281, 6391, 4011, 369, 6506, 7642, 7693, 2820...\n",
      "7       [7254, 2358, 4757, 6699, 4093, 6716, 4140, 605...\n",
      "                              ...                        \n",
      "3250    [6281, 6391, 447, 7120, 2533, 2527, 2408, 1855...\n",
      "3251    [6281, 6391, 2709, 5099, 2992, 1936, 4150, 448...\n",
      "3254    [6281, 6391, 6281, 6391, 6281, 6391, 6281, 639...\n",
      "3255    [1263, 3970, 6716, 2527, 6541, 4483, 1804, 142...\n",
      "3256    [6281, 6391, 6281, 6391, 5616, 3672, 5198, 649...\n",
      "Name: numeric_tokens, Length: 2108, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Now the df has a new column \"numeric_tokens\"\n",
    "#print(train_data.head())\n",
    "print(train_data['numeric_tokens'])\n",
    "\n",
    "## Seems alright"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d626ec",
   "metadata": {},
   "source": [
    "<b> Next step is to do padding </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e5298f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Maximum Length is: 48\n"
     ]
    }
   ],
   "source": [
    "# Figure the maximum sequence length, but since tweets are conveniently short\n",
    "# The maximum length of tweet is defined:\n",
    "MAX_SEQUENCE_LENGTH = max(train_data['numeric_tokens'].apply(len))\n",
    "print('The Maximum Length is:', MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d126ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padding is required for the dataset:\n",
    "def pad_sequence(numeric_tokens: list, max_length: int) -> list:\n",
    "    \"\"\"\n",
    "    Pads a sequence to a given length. If sequence is shorter than the target length,\n",
    "    it'll be padded with zeros.\n",
    "    \"\"\"\n",
    "    return numeric_tokens + [0]*(max_length - len(numeric_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18ca1e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the padding:\n",
    "train_data['padded_tokens'] = train_data['numeric_tokens'].apply(lambda x: pad_sequence(x, MAX_SEQUENCE_LENGTH))\n",
    "validation_data['padded_tokens'] = validation_data['numeric_tokens'].apply(lambda x: pad_sequence(x, MAX_SEQUENCE_LENGTH))\n",
    "test_data['padded_tokens'] = test_data['numeric_tokens'].apply(lambda x: pad_sequence(x, MAX_SEQUENCE_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6f9933c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1       [1269, 2851, 2118, 7359, 2559, 5508, 1162, 594...\n",
      "2       [1212, 4011, 1162, 2559, 7581, 4218, 4141, 688...\n",
      "3       [3434, 3943, 6005, 5952, 3404, 1211, 4483, 705...\n",
      "5       [6281, 6391, 4011, 369, 6506, 7642, 7693, 2820...\n",
      "7       [7254, 2358, 4757, 6699, 4093, 6716, 4140, 605...\n",
      "                              ...                        \n",
      "3250    [6281, 6391, 447, 7120, 2533, 2527, 2408, 1855...\n",
      "3251    [6281, 6391, 2709, 5099, 2992, 1936, 4150, 448...\n",
      "3254    [6281, 6391, 6281, 6391, 6281, 6391, 6281, 639...\n",
      "3255    [1263, 3970, 6716, 2527, 6541, 4483, 1804, 142...\n",
      "3256    [6281, 6391, 6281, 6391, 5616, 3672, 5198, 649...\n",
      "Name: padded_tokens, Length: 2108, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Now the df has a new column \"padded_tokens\". \n",
    "# Usually we shouldn't load all that to memory but once again, Tweets are small, so it is ok:\n",
    "#print(train_data.head())\n",
    "print(train_data['padded_tokens'])\n",
    "\n",
    "## Seems the same as previous..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edf5b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN CLASS FOR Convolutional Neural Network ###:\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx, activation_function='ReLU', pooling_strategy='max'):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs, embedding_dim))\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Activation function\n",
    "        if activation_function == 'ReLU':\n",
    "            self.activation = F.relu\n",
    "        elif activation_function == 'LeakyReLU':\n",
    "            self.activation = F.leaky_relu\n",
    "        elif activation_function == 'ELU':\n",
    "            self.activation = F.elu\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "        # Pooling strategy\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [batch size, sent len]\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #embedded = embedded.squeeze(1)\n",
    "        # embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [self.activation(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        # conved_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        if self.pooling_strategy == 'max':\n",
    "            pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        elif self.pooling_strategy == 'avg':\n",
    "            pooled = [F.avg_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported pooling strategy\")\n",
    "        # pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        # cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "008a1ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main tools for the encoding and loading:\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db9d3f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Similarly to the previous exercise, a class for the lock and loading of DataLoader:\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.tokenized_texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "793f5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to list:\n",
    "train_dataset = TextDataset(train_data['padded_tokens'].tolist(), train_data['label_number'].tolist())\n",
    "val_dataset = TextDataset(validation_data['padded_tokens'].tolist(), validation_data['label_number'].tolist())\n",
    "test_dataset = TextDataset(test_data['padded_tokens'].tolist(), test_data['label_number'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6db47cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Batch size to work with: GS of 32, 64 and 128:\n",
    "batch_size = 32\n",
    "\n",
    "#Loaded and ready:\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b0b68f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001BC84637850>\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a250718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device gpu or cpu is: cpu\n"
     ]
    }
   ],
   "source": [
    "# Execute on CPU since the small size of Tweets should make this easy:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('The device gpu or cpu is:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93cfc5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7749\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Main hyperparameters:\n",
    "vocab_size = len(word_to_index) + 1  # +1 for padding \n",
    "print(vocab_size)\n",
    "embedding_dim = 300  \n",
    "n_filters = 110\n",
    "filter_sizes = [2,3,4]\n",
    "#output_dim = len(train_data['label_number'].unique())\n",
    "output_dim = 1\n",
    "print(output_dim)\n",
    "dropout=0.1\n",
    "#dropout=0.24691844248854944\n",
    "pad_idx = 0  # Assuming 0 is the index for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4febf882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7749\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "### Alternative setting for applying the Optima best settings for Sadness and Joy dataset:\n",
    "# Main hyperparameters:\n",
    "vocab_size = len(word_to_index) + 1  # +1 for padding \n",
    "print(vocab_size)\n",
    "embedding_dim = 300  \n",
    "n_filters = 130\n",
    "filter_sizes = [2,3,4]\n",
    "#output_dim = len(train_data['label_number'].unique())\n",
    "output_dim = 1\n",
    "print(output_dim)\n",
    "dropout=0.30216765316720884\n",
    "pad_idx = 0  # Assuming 0 is the index for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1336d502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's embedding layer size: torch.Size([7749, 300])\n"
     ]
    }
   ],
   "source": [
    "## Run the main CNN model builder:\n",
    "model = TextCNN(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx).to(device)\n",
    "print(\"Model's embedding layer size:\", model.embedding.weight.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70b44fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's embedding layer size: torch.Size([7749, 300])\n"
     ]
    }
   ],
   "source": [
    "## Alternate for the CNN model builder for Optima Settings for Sadjoy:\n",
    "model_optima_sadjoy = TextCNN(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx).to(device)\n",
    "print(\"Model's embedding layer size:\", model_optima_sadjoy.embedding.weight.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ce9ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.embedding.weight.data.copy_(torch.tensor(embedding_matrix))\n",
    "model.embedding.weight.requires_grad = False  # Freeze the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce26de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternate for optima sadjoy:\n",
    "model_optima_sadjoy.embedding.weight.requires_grad = False  # Freeze the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f803512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "## BCE loss is used since it's only two categories\n",
    "criterion = nn.BCELoss().to(device)\n",
    "#criterion = nn.CrossEntropyLoss().to(device)\n",
    "#lr=0.000682446937962706\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f88b475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer for sadjoy optima config:\n",
    "\n",
    "## BCE loss is used since it's only two categories\n",
    "criterion = nn.BCELoss().to(device)\n",
    "#criterion = nn.CrossEntropyLoss().to(device)\n",
    "#lr=0.000682446937962706\n",
    "optimizer = torch.optim.Adam(model_optima_sadjoy.parameters(), lr=0.0007809671359222956)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12354ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main function to perform training:\n",
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_texts, batch_labels in iterator:\n",
    "        batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "        batch_labels = batch_labels.view(-1, 1)\n",
    "        # Convert the labels to float since its now only 2 categories for sadness and joy:\n",
    "        batch_labels = batch_labels.float()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_texts)\n",
    "        # Apply sigmoid activation to the predictions to compress to either 0, 1\n",
    "        predictions = torch.sigmoid(predictions)\n",
    "        loss = criterion(predictions, batch_labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a707272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for evaluation and metric scores:\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_texts, batch_labels in iterator:\n",
    "            batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "            # Reshape the labels to [batch_size, 1]\n",
    "            batch_labels = batch_labels.view(-1, 1)\n",
    "            # Cast the labels to float\n",
    "            batch_labels = batch_labels.float()\n",
    "            predictions = model(batch_texts)\n",
    "            # Apply sigmoid activation to the predictions\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "            loss = criterion(predictions, batch_labels)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3aff4a91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 1.003\n",
      "\t Val. Loss: 0.667\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.374\n",
      "\t Val. Loss: 0.740\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.269\n",
      "\t Val. Loss: 2.171\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.486\n",
      "\t Val. Loss: 2.449\n",
      "Epoch: 05\n",
      "\tTrain Loss: 1.021\n",
      "\t Val. Loss: 3.906\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.818\n",
      "\t Val. Loss: 3.487\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.449\n",
      "\t Val. Loss: 3.835\n",
      "Epoch: 08\n",
      "\tTrain Loss: 1.007\n",
      "\t Val. Loss: 4.670\n",
      "Epoch: 09\n",
      "\tTrain Loss: 1.143\n",
      "\t Val. Loss: 5.624\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.639\n",
      "\t Val. Loss: 7.256\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.981\n",
      "\t Val. Loss: 7.832\n",
      "Epoch: 12\n",
      "\tTrain Loss: 1.038\n",
      "\t Val. Loss: 8.242\n",
      "Epoch: 13\n",
      "\tTrain Loss: 1.013\n",
      "\t Val. Loss: 8.673\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.973\n",
      "\t Val. Loss: 10.629\n",
      "Epoch: 15\n",
      "\tTrain Loss: 1.381\n",
      "\t Val. Loss: 8.474\n",
      "Epoch: 16\n",
      "\tTrain Loss: 1.039\n",
      "\t Val. Loss: 10.696\n",
      "Epoch: 17\n",
      "\tTrain Loss: 3.416\n",
      "\t Val. Loss: 22.250\n",
      "Epoch: 18\n",
      "\tTrain Loss: 1.999\n",
      "\t Val. Loss: 15.437\n",
      "Epoch: 19\n",
      "\tTrain Loss: 1.782\n",
      "\t Val. Loss: 20.173\n",
      "Epoch: 20\n",
      "\tTrain Loss: 2.274\n",
      "\t Val. Loss: 15.420\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs at 20\n",
    "n_epochs = 20\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    valid_loss = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'sadjoy_model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a15d1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.186\n",
      "\t Val. Loss: 0.473\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.102\n",
      "\t Val. Loss: 0.476\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.058\n",
      "\t Val. Loss: 0.456\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.048\n",
      "\t Val. Loss: 0.488\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.032\n",
      "\t Val. Loss: 0.464\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.033\n",
      "\t Val. Loss: 0.517\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.028\n",
      "\t Val. Loss: 0.480\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.018\n",
      "\t Val. Loss: 0.490\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.019\n",
      "\t Val. Loss: 0.484\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.014\n",
      "\t Val. Loss: 0.573\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.017\n",
      "\t Val. Loss: 0.536\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.013\n",
      "\t Val. Loss: 0.563\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.008\n",
      "\t Val. Loss: 0.500\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.019\n",
      "\t Val. Loss: 0.508\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.010\n",
      "\t Val. Loss: 0.545\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.011\n",
      "\t Val. Loss: 0.531\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.006\n",
      "\t Val. Loss: 0.518\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.012\n",
      "\t Val. Loss: 0.528\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.013\n",
      "\t Val. Loss: 0.540\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.010\n",
      "\t Val. Loss: 0.567\n"
     ]
    }
   ],
   "source": [
    "### Training Sadjoy Optima model ###\n",
    "\n",
    "# Number of epochs at 20\n",
    "n_epochs = 20\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model_optima_sadjoy, train_loader, optimizer, criterion, device)\n",
    "    valid_loss = evaluate(model_optima_sadjoy, val_loader, criterion, device)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_optima_sadjoy.state_dict(), 'sadjoy_model_plus_optima.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "863b311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The train loss decreases drastically\n",
    "#### Now we do the metric evaluations for the base model with no hyperparameter changes:\n",
    "## To evaluate the model and find the f1 score and accuracy\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            text, labels = batch\n",
    "            text, labels = text.to(device), labels.to(device)  # Move data to device\n",
    "            # Reshape the labels to [batch_size, 1]\n",
    "            labels = labels.view(-1)\n",
    "            # Cast the labels to float\n",
    "            labels = labels.float()\n",
    "            predictions = model(text).squeeze(1)\n",
    "            # Apply sigmoid activation to the predictions\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Convert predictions to class labels and append to lists\n",
    "            predicted_labels = (predictions.round().long())\n",
    "            all_predictions.extend(predicted_labels.tolist())\n",
    "            all_true_labels.extend(labels.tolist())\n",
    "\n",
    "    return all_predictions, all_true_labels, total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8034fb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save the model:\n",
    "torch.save(model, 'sadjoymodel.pth')\n",
    "## Load the base initial model for sadjoy:\n",
    "model.load_state_dict(torch.load('sadjoy_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcc75cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save the alternative optima settings model for sadjoy:\n",
    "torch.save(model_optima_sadjoy, 'sadjoymodel_plus_optima.pth')\n",
    "## Load the base initial model for sadjoy:\n",
    "model_optima_sadjoy.load_state_dict(torch.load('sadjoy_model_plus_optima.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4a0adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All predictions, labels and testing loss values\n",
    "all_preds, all_labels, test_loss = evaluate_model(model, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5314721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All predictions, labels and testing loss values\n",
    "\n",
    "### Alternative for optima sadjoy:\n",
    "all_preds_alt, all_labels_alt, test_loss_alt = evaluate_model(model_optima_sadjoy, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab775fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.671\n",
      "F1 Score: 0.665\n"
     ]
    }
   ],
   "source": [
    "#### Now we calculate accuracy and F1 score for the base version of SadJoy model:\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average='macro') # 'macro' calculates metrics for each label and then does an unweighted mean\n",
    "\n",
    "#precision = precision_score(all_labels, all_preds, average='macro')\n",
    "#recall = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97023800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.757\n",
      "F1 Score: 0.736\n"
     ]
    }
   ],
   "source": [
    "#Alternative for optima sadjoy:\n",
    "#### Now we calculate accuracy and F1 score for SadJoy with Optima:\n",
    "accuracy_alt = accuracy_score(all_labels_alt, all_preds_alt)\n",
    "f1_alt = f1_score(all_labels_alt, all_preds_alt, average='macro') # 'macro' calculates metrics for each label and then does an unweighted mean\n",
    "\n",
    "#precision = precision_score(all_labels, all_preds, average='macro')\n",
    "#recall = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy_alt:.3f}\")\n",
    "print(f\"F1 Score: {f1_alt:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f12dafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First set of experiments:\n",
    "## Hyperparameter changes: increase in batch size and filter increase\n",
    "## Batch size to work with: GS of 32, 64 and 128:\n",
    "### Batch size INCREASED from 32 to 64:\n",
    "batch_size = 64\n",
    "\n",
    "#Loaded and ready:\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a230184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7749\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Main hyperparameters:\n",
    "## We adjust the \n",
    "vocab_size = len(word_to_index) + 1  # +1 for padding \n",
    "print(vocab_size)\n",
    "embedding_dim = 300  \n",
    "## Number of filter INCREASED from 110 to 210:\n",
    "n_filters = 210\n",
    "filter_sizes = [2,3,4]\n",
    "#output_dim = len(train_data['label_number'].unique())\n",
    "output_dim = 1\n",
    "print(output_dim)\n",
    "dropout=0.1\n",
    "#dropout=0.24691844248854944\n",
    "pad_idx = 0  # Assuming 0 is the index for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ca31591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's embedding layer size: torch.Size([7749, 300])\n"
     ]
    }
   ],
   "source": [
    "## Run the main CNN model builder:\n",
    "model_alt_one = TextCNN(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx).to(device)\n",
    "print(\"Model's embedding layer size:\", model_alt_one.embedding.weight.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e81517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.embedding.weight.data.copy_(torch.tensor(embedding_matrix))\n",
    "model_alt_one.embedding.weight.requires_grad = False  # Freeze the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab90ba07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.642\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.640\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.645\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.643\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.644\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.640\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.643\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.642\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.640\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.638\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.642\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.637\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.641\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.640\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.640\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.644\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.644\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.639\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.639\n",
      "\t Val. Loss: 0.603\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.637\n",
      "\t Val. Loss: 0.603\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs at 20\n",
    "n_epochs = 20\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model_alt_one, train_loader, optimizer, criterion, device)\n",
    "    valid_loss = evaluate(model_alt_one, val_loader, criterion, device)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_alt_one.state_dict(), 'sadjoy_model_alt_1.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab202360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save the model:\n",
    "torch.save(model_alt_one, 'sadjoymodel_alt_one.pth')\n",
    "## Load the base initial model for sadjoy:\n",
    "model_alt_one.load_state_dict(torch.load('sadjoy_model_alt_1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "902c2fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All predictions, labels and testing loss values\n",
    "all_preds, all_labels, test_loss = evaluate_model(model_alt_one, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9f5f29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.609\n",
      "F1 Score: 0.384\n"
     ]
    }
   ],
   "source": [
    "#### Now we calculate accuracy and F1 score for the first alternative experiment of SadJoy model:\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average='macro') # 'macro' calculates metrics for each label and then does an unweighted mean\n",
    "\n",
    "#precision = precision_score(all_labels, all_preds, average='macro')\n",
    "#recall = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4570b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Second set of experiments:\n",
    "### Learning rate increased and epoch number increased:\n",
    "\n",
    "## First we restore batch size to GS 32:\n",
    "## Batch size to work with: GS of 32, 64 and 128:\n",
    "batch_size = 32\n",
    "\n",
    "#Loaded and ready:\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d050a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7749\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Main hyperparameters:\n",
    "vocab_size = len(word_to_index) + 1  # +1 for padding \n",
    "print(vocab_size)\n",
    "embedding_dim = 300  \n",
    "n_filters = 110\n",
    "filter_sizes = [2,3,4]\n",
    "#output_dim = len(train_data['label_number'].unique())\n",
    "output_dim = 1\n",
    "print(output_dim)\n",
    "dropout=0.1\n",
    "#dropout=0.24691844248854944\n",
    "pad_idx = 0  # Assuming 0 is the index for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dea5fa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's embedding layer size: torch.Size([7749, 300])\n"
     ]
    }
   ],
   "source": [
    "## Run the main CNN model builder again for model alternative 2:\n",
    "model_alt_two = TextCNN(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx).to(device)\n",
    "print(\"Model's embedding layer size:\", model_alt_two.embedding.weight.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78e74ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "## BCE loss is used since it's only two categories\n",
    "criterion = nn.BCELoss().to(device)\n",
    "#criterion = nn.CrossEntropyLoss().to(device)\n",
    "#lr=0.000682446937962706\n",
    "### Learning rate is increase to 0.05\n",
    "optimizer = torch.optim.Adam(model_alt_two.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "933b05d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 33.231\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 02\n",
      "\tTrain Loss: 33.597\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 03\n",
      "\tTrain Loss: 33.617\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 04\n",
      "\tTrain Loss: 33.604\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 05\n",
      "\tTrain Loss: 33.590\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 06\n",
      "\tTrain Loss: 33.563\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 07\n",
      "\tTrain Loss: 33.577\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 08\n",
      "\tTrain Loss: 33.611\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 09\n",
      "\tTrain Loss: 33.584\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 10\n",
      "\tTrain Loss: 33.570\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 11\n",
      "\tTrain Loss: 33.577\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 12\n",
      "\tTrain Loss: 33.557\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 13\n",
      "\tTrain Loss: 33.617\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 14\n",
      "\tTrain Loss: 33.590\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 15\n",
      "\tTrain Loss: 33.584\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 16\n",
      "\tTrain Loss: 33.584\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 17\n",
      "\tTrain Loss: 33.590\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 18\n",
      "\tTrain Loss: 33.550\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 19\n",
      "\tTrain Loss: 33.611\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 20\n",
      "\tTrain Loss: 33.604\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 21\n",
      "\tTrain Loss: 33.617\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 22\n",
      "\tTrain Loss: 33.584\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 23\n",
      "\tTrain Loss: 33.617\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 24\n",
      "\tTrain Loss: 33.604\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 25\n",
      "\tTrain Loss: 33.563\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 26\n",
      "\tTrain Loss: 33.570\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 27\n",
      "\tTrain Loss: 33.604\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 28\n",
      "\tTrain Loss: 33.550\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 29\n",
      "\tTrain Loss: 33.590\n",
      "\t Val. Loss: 33.681\n",
      "Epoch: 30\n",
      "\tTrain Loss: 33.584\n",
      "\t Val. Loss: 33.681\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs INCREASED to 30:\n",
    "n_epochs = 30\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model_alt_two, train_loader, optimizer, criterion, device)\n",
    "    valid_loss = evaluate(model_alt_two, val_loader, criterion, device)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_alt_two.state_dict(), 'sadjoy_model_alt2.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b6e0c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save the model:\n",
    "torch.save(model_alt_two, 'sadjoymodel_alt_two.pth')\n",
    "## Load the base initial model for sadjoy:\n",
    "model_alt_two.load_state_dict(torch.load('sadjoy_model_alt2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b855a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All predictions, labels and testing loss values\n",
    "all_preds, all_labels, test_loss = evaluate_model(model_alt_two, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b76df3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.609\n",
      "F1 Score: 0.379\n"
     ]
    }
   ],
   "source": [
    "#### Now we calculate accuracy and F1 score for the second experiment set of the SadJoy model:\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average='macro') # 'macro' calculates metrics for each label and then does an unweighted mean\n",
    "\n",
    "#precision = precision_score(all_labels, all_preds, average='macro')\n",
    "#recall = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b7c08d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Third (last) set of experiments:\n",
    "### Different optimizer and dropout increased:\n",
    "## Batch size to work with: GS of 32, 64 and 128:\n",
    "batch_size = 32\n",
    "\n",
    "#Loaded and ready:\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b29ec07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7749\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Main hyperparameters for experiment 3:\n",
    "vocab_size = len(word_to_index) + 1  # +1 for padding \n",
    "print(vocab_size)\n",
    "embedding_dim = 300  \n",
    "n_filters = 110\n",
    "filter_sizes = [2,3,4]\n",
    "#output_dim = len(train_data['label_number'].unique())\n",
    "output_dim = 1\n",
    "print(output_dim)\n",
    "#### Dropout INCREASED from 0.1 to 0.2\n",
    "dropout=0.2\n",
    "#dropout=0.24691844248854944\n",
    "pad_idx = 0  # Assuming 0 is the index for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49ae78c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's embedding layer size: torch.Size([7749, 300])\n"
     ]
    }
   ],
   "source": [
    "## Run the main CNN model builder again for model alternative 2:\n",
    "model_alt_three = TextCNN(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx).to(device)\n",
    "print(\"Model's embedding layer size:\", model_alt_three.embedding.weight.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b351df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "## BCE loss is used since it's only two categories\n",
    "criterion = nn.BCELoss().to(device)\n",
    "#criterion = nn.CrossEntropyLoss().to(device)\n",
    "#lr=0.000682446937962706\n",
    "#### Optimizer changed to SGD\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91e22142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.643\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.647\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.643\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.644\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.649\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.645\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.639\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.642\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.644\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.642\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.643\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.644\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.640\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.635\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.642\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.641\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.642\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.642\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.641\n",
      "\t Val. Loss: 0.623\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.644\n",
      "\t Val. Loss: 0.623\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs back to 20:\n",
    "n_epochs = 20\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model_alt_three, train_loader, optimizer, criterion, device)\n",
    "    valid_loss = evaluate(model_alt_three, val_loader, criterion, device)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_alt_three.state_dict(), 'sadjoy_model_alt3.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9d3b70ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save the model:\n",
    "torch.save(model_alt_three, 'sadjoymodel_alt_three.pth')\n",
    "## Load the base initial model for sadjoy:\n",
    "model_alt_three.load_state_dict(torch.load('sadjoy_model_alt3.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "535dbfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All predictions, labels and testing loss values\n",
    "all_preds, all_labels, test_loss = evaluate_model(model_alt_three, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "641bf3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.610\n",
      "F1 Score: 0.387\n"
     ]
    }
   ],
   "source": [
    "#### Now we calculate accuracy and F1 score for the third last experiment set of the SadJoy model:\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average='macro') # 'macro' calculates metrics for each label and then does an unweighted mean\n",
    "\n",
    "#precision = precision_score(all_labels, all_preds, average='macro')\n",
    "#recall = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c5146f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-09 10:00:11,743] A new study created in memory with name: no-name-427c8648-16e2-4e4b-a4ed-c51135f55009\n",
      "[I 2023-11-09 10:00:33,210] Trial 0 finished with value: 0.5087175882524915 and parameters: {'lr': 0.0008722720369465334, 'dropout': 0.5319940062916195, 'n_filters': 100, 'filter_sizes': [3, 4, 5], 'embedding_dim': 100, 'weight_decay': 0.00028948772875134015, 'batch_size': 64, 'optimizer_type': 'RMSProp', 'activation_function': 'ELU', 'pooling_strategy': 'max'}. Best is trial 0 with value: 0.5087175882524915.\n",
      "[I 2023-11-09 10:01:20,747] Trial 1 finished with value: 0.6324671573109097 and parameters: {'lr': 0.0005562504815944998, 'dropout': 0.577547835208084, 'n_filters': 120, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0025162111599918873, 'batch_size': 64, 'optimizer_type': 'SGD', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'max'}. Best is trial 0 with value: 0.5087175882524915.\n",
      "[I 2023-11-09 10:01:53,498] Trial 2 finished with value: 0.5297843250963423 and parameters: {'lr': 0.0005659987390514193, 'dropout': 0.12656175707986636, 'n_filters': 70, 'filter_sizes': [4, 5, 6], 'embedding_dim': 100, 'weight_decay': 0.031602686632719895, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'max'}. Best is trial 0 with value: 0.5087175882524915.\n",
      "[I 2023-11-09 10:02:17,915] Trial 3 finished with value: 0.5693468285931481 and parameters: {'lr': 0.00045001123827120734, 'dropout': 0.24670343177740567, 'n_filters': 60, 'filter_sizes': [2, 3, 4], 'embedding_dim': 100, 'weight_decay': 0.0021444664891066153, 'batch_size': 64, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 0 with value: 0.5087175882524915.\n",
      "[I 2023-11-09 10:02:52,033] Trial 4 finished with value: 0.6138349076112112 and parameters: {'lr': 7.434212612316713e-05, 'dropout': 0.6228292774462093, 'n_filters': 140, 'filter_sizes': [3, 4, 5], 'embedding_dim': 100, 'weight_decay': 0.015500426381668611, 'batch_size': 16, 'optimizer_type': 'RMSProp', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 0 with value: 0.5087175882524915.\n",
      "[I 2023-11-09 10:03:21,529] Trial 5 finished with value: 0.4997180605100261 and parameters: {'lr': 0.0005233699329827366, 'dropout': 0.3591866705147042, 'n_filters': 70, 'filter_sizes': [3, 4, 5], 'embedding_dim': 100, 'weight_decay': 0.0003068094228968884, 'batch_size': 16, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 5 with value: 0.4997180605100261.\n",
      "[I 2023-11-09 10:04:43,048] Trial 6 finished with value: 0.5668215867545869 and parameters: {'lr': 5.539912125411514e-05, 'dropout': 0.599225214823583, 'n_filters': 150, 'filter_sizes': [3, 4, 5], 'embedding_dim': 200, 'weight_decay': 0.00019415832181661615, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ReLU', 'pooling_strategy': 'max'}. Best is trial 5 with value: 0.4997180605100261.\n",
      "[I 2023-11-09 10:05:10,363] Trial 7 finished with value: 0.5212277687258191 and parameters: {'lr': 0.0004032973763057832, 'dropout': 0.3674683198043567, 'n_filters': 90, 'filter_sizes': [3, 4, 5], 'embedding_dim': 100, 'weight_decay': 0.0008849808207729735, 'batch_size': 128, 'optimizer_type': 'RMSProp', 'activation_function': 'ReLU', 'pooling_strategy': 'max'}. Best is trial 5 with value: 0.4997180605100261.\n",
      "[I 2023-11-09 10:06:59,200] Trial 8 finished with value: 0.5382796161704593 and parameters: {'lr': 3.363198931126475e-05, 'dropout': 0.15609938766556233, 'n_filters': 130, 'filter_sizes': [3, 4, 5], 'embedding_dim': 300, 'weight_decay': 0.0004049182955750334, 'batch_size': 32, 'optimizer_type': 'RMSProp', 'activation_function': 'ReLU', 'pooling_strategy': 'max'}. Best is trial 5 with value: 0.4997180605100261.\n",
      "[I 2023-11-09 10:07:48,322] Trial 9 finished with value: 0.6441985103819106 and parameters: {'lr': 0.00013157483552367582, 'dropout': 0.24755372087952657, 'n_filters': 90, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 1.1636849315627259e-05, 'batch_size': 32, 'optimizer_type': 'SGD', 'activation_function': 'ReLU', 'pooling_strategy': 'max'}. Best is trial 5 with value: 0.4997180605100261.\n",
      "[I 2023-11-09 10:09:10,408] Trial 10 finished with value: 0.6190945969687568 and parameters: {'lr': 1.1746004157713386e-05, 'dropout': 0.4472439334507559, 'n_filters': 50, 'filter_sizes': [4, 5, 6], 'embedding_dim': 300, 'weight_decay': 4.119184289065083e-05, 'batch_size': 16, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 5 with value: 0.4997180605100261.\n",
      "[I 2023-11-09 10:09:33,669] Trial 11 finished with value: 0.5255994333161248 and parameters: {'lr': 0.000866710010844951, 'dropout': 0.46815644219849667, 'n_filters': 110, 'filter_sizes': [3, 4, 5], 'embedding_dim': 100, 'weight_decay': 0.00012587306355717415, 'batch_size': 64, 'optimizer_type': 'RMSProp', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 5 with value: 0.4997180605100261.\n",
      "[I 2023-11-09 10:10:06,931] Trial 12 finished with value: 0.5292137120332983 and parameters: {'lr': 0.000983551272362478, 'dropout': 0.6924364721021112, 'n_filters': 80, 'filter_sizes': [3, 4, 5], 'embedding_dim': 100, 'weight_decay': 0.00012943920778301442, 'batch_size': 16, 'optimizer_type': 'RMSProp', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 5 with value: 0.4997180605100261.\n",
      "[I 2023-11-09 10:10:44,756] Trial 13 finished with value: 0.6070840623643663 and parameters: {'lr': 0.00021712358635042853, 'dropout': 0.38915078330749253, 'n_filters': 100, 'filter_sizes': [3, 4, 5], 'embedding_dim': 100, 'weight_decay': 0.0005939280301947974, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 5 with value: 0.4997180605100261.\n",
      "[I 2023-11-09 10:11:55,244] Trial 14 finished with value: 0.6483471261130439 and parameters: {'lr': 0.00024848787157942477, 'dropout': 0.5033309319570703, 'n_filters': 70, 'filter_sizes': [3, 4, 5], 'embedding_dim': 300, 'weight_decay': 0.003920778677615421, 'batch_size': 64, 'optimizer_type': 'SGD', 'activation_function': 'ELU', 'pooling_strategy': 'max'}. Best is trial 5 with value: 0.4997180605100261.\n",
      "[I 2023-11-09 10:12:52,106] Trial 15 finished with value: 0.5227009140782886 and parameters: {'lr': 0.0009376445680715327, 'dropout': 0.3418866540303305, 'n_filters': 110, 'filter_sizes': [4, 5, 6], 'embedding_dim': 100, 'weight_decay': 4.77060305060851e-05, 'batch_size': 16, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 5 with value: 0.4997180605100261.\n",
      "[I 2023-11-09 10:13:53,976] Trial 16 finished with value: 0.5834977312220467 and parameters: {'lr': 0.0002550252312361151, 'dropout': 0.5102144964024392, 'n_filters': 80, 'filter_sizes': [3, 4, 5], 'embedding_dim': 100, 'weight_decay': 0.0003228670787554172, 'batch_size': 64, 'optimizer_type': 'RMSProp', 'activation_function': 'ELU', 'pooling_strategy': 'max'}. Best is trial 5 with value: 0.4997180605100261.\n",
      "[I 2023-11-09 10:14:56,772] Trial 17 finished with value: 0.613438841369417 and parameters: {'lr': 0.00015080975228516283, 'dropout': 0.42715242165290573, 'n_filters': 60, 'filter_sizes': [3, 4, 5], 'embedding_dim': 100, 'weight_decay': 0.08224249995547459, 'batch_size': 16, 'optimizer_type': 'RMSProp', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 5 with value: 0.4997180605100261.\n",
      "[I 2023-11-09 10:16:57,785] Trial 18 finished with value: 0.508604938785235 and parameters: {'lr': 0.0003518438387370139, 'dropout': 0.5355137532743834, 'n_filters': 100, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0014611324269179053, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'max'}. Best is trial 5 with value: 0.4997180605100261.\n",
      "[I 2023-11-09 10:18:11,410] Trial 19 finished with value: 0.46767764869663453 and parameters: {'lr': 0.00032992214044606837, 'dropout': 0.320612569793929, 'n_filters': 50, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.001562360388016454, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 19 with value: 0.46767764869663453.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-09 10:19:20,609] Trial 20 finished with value: 0.4915858167741034 and parameters: {'lr': 0.0001950511965164039, 'dropout': 0.3075190348382899, 'n_filters': 50, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.009179754645275687, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 19 with value: 0.46767764869663453.\n",
      "[I 2023-11-09 10:20:30,452] Trial 21 finished with value: 0.5322356099883715 and parameters: {'lr': 0.00017171977289842796, 'dropout': 0.31937351062925834, 'n_filters': 50, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0073441153634759065, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 19 with value: 0.46767764869663453.\n",
      "[I 2023-11-09 10:21:41,485] Trial 22 finished with value: 0.5036296836204 and parameters: {'lr': 0.00029417770805633824, 'dropout': 0.30625406810096645, 'n_filters': 50, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0010317214746503294, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 19 with value: 0.46767764869663453.\n",
      "[I 2023-11-09 10:22:30,730] Trial 23 finished with value: 0.514155184229215 and parameters: {'lr': 0.00035229699269873673, 'dropout': 0.4024335187540307, 'n_filters': 60, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.004772427571706061, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 19 with value: 0.46767764869663453.\n",
      "[I 2023-11-09 10:23:45,461] Trial 24 finished with value: 0.509827239645852 and parameters: {'lr': 0.00019958085620636436, 'dropout': 0.2839309203918583, 'n_filters': 70, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.010901680122658438, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 19 with value: 0.46767764869663453.\n",
      "[I 2023-11-09 10:24:50,722] Trial 25 finished with value: 0.5639707412984636 and parameters: {'lr': 0.00010956783637550064, 'dropout': 0.36296612940914086, 'n_filters': 50, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.002925603410805894, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 19 with value: 0.46767764869663453.\n",
      "[I 2023-11-09 10:25:49,633] Trial 26 finished with value: 0.5047290904654397 and parameters: {'lr': 0.0006333801203298961, 'dropout': 0.21223337870120568, 'n_filters': 60, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0013897907962472167, 'batch_size': 16, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 19 with value: 0.46767764869663453.\n",
      "[I 2023-11-09 10:28:02,083] Trial 27 finished with value: 0.6343576245837741 and parameters: {'lr': 0.0002868625208796157, 'dropout': 0.3326287890058828, 'n_filters': 80, 'filter_sizes': [4, 5, 6], 'embedding_dim': 300, 'weight_decay': 0.0008257548624979893, 'batch_size': 128, 'optimizer_type': 'SGD', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 19 with value: 0.46767764869663453.\n",
      "[I 2023-11-09 10:29:24,649] Trial 28 finished with value: 0.4692385138736831 and parameters: {'lr': 0.00017685059961505194, 'dropout': 0.2837540671730068, 'n_filters': 70, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.004476640204589707, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 19 with value: 0.46767764869663453.\n",
      "[I 2023-11-09 10:30:30,405] Trial 29 finished with value: 0.5296338001887003 and parameters: {'lr': 0.0001664731386189458, 'dropout': 0.2707009567410785, 'n_filters': 50, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.006700993286122268, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 19 with value: 0.46767764869663453.\n",
      "[I 2023-11-09 10:31:44,281] Trial 30 finished with value: 0.5024787642889552 and parameters: {'lr': 0.00011905467395859397, 'dropout': 0.17005036958409586, 'n_filters': 60, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.01251969889458928, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 19 with value: 0.46767764869663453.\n",
      "[I 2023-11-09 10:33:03,305] Trial 31 finished with value: 0.6043042441209158 and parameters: {'lr': 0.0002293951396603143, 'dropout': 0.29485817783400264, 'n_filters': 70, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0027595441414679147, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 19 with value: 0.46767764869663453.\n",
      "[I 2023-11-09 10:34:24,771] Trial 32 finished with value: 0.4716953742835257 and parameters: {'lr': 0.00047346793296548805, 'dropout': 0.38984293515374274, 'n_filters': 90, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0017611424682256055, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 19 with value: 0.46767764869663453.\n",
      "[I 2023-11-09 10:35:47,012] Trial 33 finished with value: 0.46805650037195945 and parameters: {'lr': 0.00034343103548026044, 'dropout': 0.4039416844269493, 'n_filters': 90, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0018696595869704539, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 19 with value: 0.46767764869663453.\n",
      "[I 2023-11-09 10:36:53,320] Trial 34 finished with value: 0.6320109102461073 and parameters: {'lr': 0.00045653526774399266, 'dropout': 0.40230323482219965, 'n_filters': 90, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0020311074769516608, 'batch_size': 128, 'optimizer_type': 'SGD', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 19 with value: 0.46767764869663453.\n",
      "[I 2023-11-09 10:38:24,645] Trial 35 finished with value: 0.4274591205434667 and parameters: {'lr': 0.0006205437298431645, 'dropout': 0.4282066838333107, 'n_filters': 110, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0041513371348512315, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 10:40:18,548] Trial 36 finished with value: 0.5045487615797255 and parameters: {'lr': 0.0007112884415813235, 'dropout': 0.4452275567687969, 'n_filters': 110, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.020266040667362515, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 10:42:17,636] Trial 37 finished with value: 0.43567070323559975 and parameters: {'lr': 0.0006053533778995406, 'dropout': 0.23767292169838108, 'n_filters': 120, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.004649985234950364, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 10:44:24,820] Trial 38 finished with value: 0.47927617985341286 and parameters: {'lr': 0.000590156520000664, 'dropout': 0.22213757300738685, 'n_filters': 120, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.002483420420142694, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 10:47:06,551] Trial 39 finished with value: 0.493783396979173 and parameters: {'lr': 0.0007077502997756735, 'dropout': 0.10538339054333909, 'n_filters': 130, 'filter_sizes': [4, 5, 6], 'embedding_dim': 200, 'weight_decay': 0.005746922125256387, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-09 10:48:43,811] Trial 40 finished with value: 0.6382001539071401 and parameters: {'lr': 0.00038846710068263083, 'dropout': 0.34834187483448265, 'n_filters': 120, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.004262904365149423, 'batch_size': 128, 'optimizer_type': 'SGD', 'activation_function': 'ReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 10:50:58,650] Trial 41 finished with value: 0.4797075374258889 and parameters: {'lr': 0.0005555784757515292, 'dropout': 0.2468881273955814, 'n_filters': 130, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0036613802736826235, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 10:52:50,640] Trial 42 finished with value: 0.48701101707087624 and parameters: {'lr': 0.00031246926833652705, 'dropout': 0.2753183024941234, 'n_filters': 110, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.006729760362519028, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 10:54:48,708] Trial 43 finished with value: 0.4734562544359101 and parameters: {'lr': 0.00046735777553607634, 'dropout': 0.33183331269122546, 'n_filters': 120, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.001974910550444667, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 10:57:09,509] Trial 44 finished with value: 0.5052132656176885 and parameters: {'lr': 0.00034595939296666166, 'dropout': 0.36893623364492634, 'n_filters': 140, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.0011306043153013069, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 10:58:58,879] Trial 45 finished with value: 0.5391451708144612 and parameters: {'lr': 0.00041941223982920573, 'dropout': 0.4178214953852831, 'n_filters': 100, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0032401207560981845, 'batch_size': 64, 'optimizer_type': 'Adam', 'activation_function': 'ReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:00:13,361] Trial 46 finished with value: 0.49722565710544586 and parameters: {'lr': 0.0008045532238530907, 'dropout': 0.36934565349938925, 'n_filters': 80, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0005947197813041909, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'max'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:02:02,669] Trial 47 finished with value: 0.4766795030898518 and parameters: {'lr': 0.000547055931393867, 'dropout': 0.32285849757183666, 'n_filters': 100, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.01720171518157145, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:05:32,000] Trial 48 finished with value: 0.45287510338756776 and parameters: {'lr': 0.000745057963942208, 'dropout': 0.27041340627715793, 'n_filters': 140, 'filter_sizes': [4, 5, 6], 'embedding_dim': 300, 'weight_decay': 0.005183537345622542, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:10:11,821] Trial 49 finished with value: 0.5017256240049998 and parameters: {'lr': 0.0007329560105863648, 'dropout': 0.47057222557245637, 'n_filters': 150, 'filter_sizes': [4, 5, 6], 'embedding_dim': 300, 'weight_decay': 0.0016476560057807313, 'batch_size': 32, 'optimizer_type': 'RMSProp', 'activation_function': 'ReLU', 'pooling_strategy': 'max'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:13:18,597] Trial 50 finished with value: 0.6170426342222426 and parameters: {'lr': 0.0008010522265217581, 'dropout': 0.21560676859406303, 'n_filters': 150, 'filter_sizes': [4, 5, 6], 'embedding_dim': 300, 'weight_decay': 0.008249292223192824, 'batch_size': 32, 'optimizer_type': 'SGD', 'activation_function': 'ReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:16:36,692] Trial 51 finished with value: 0.5352609861228201 and parameters: {'lr': 0.0005916029437252581, 'dropout': 0.26253392500404893, 'n_filters': 140, 'filter_sizes': [4, 5, 6], 'embedding_dim': 300, 'weight_decay': 0.004524911635905022, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:19:38,815] Trial 52 finished with value: 0.5221895782483948 and parameters: {'lr': 0.0004939588461803249, 'dropout': 0.2938207597678174, 'n_filters': 90, 'filter_sizes': [4, 5, 6], 'embedding_dim': 300, 'weight_decay': 0.002559067738285642, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:23:17,940] Trial 53 finished with value: 0.47634462722473675 and parameters: {'lr': 0.0009220016468435237, 'dropout': 0.24936331343424833, 'n_filters': 140, 'filter_sizes': [4, 5, 6], 'embedding_dim': 300, 'weight_decay': 0.005750847498893018, 'batch_size': 64, 'optimizer_type': 'Adam', 'activation_function': 'ReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:25:11,356] Trial 54 finished with value: 0.47546882347928154 and parameters: {'lr': 0.00038113365135250066, 'dropout': 0.308681704816085, 'n_filters': 110, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.00337198455450424, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:26:42,444] Trial 55 finished with value: 0.4757578766180409 and parameters: {'lr': 0.0002696435645417799, 'dropout': 0.3523024773821525, 'n_filters': 130, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.01039975500728029, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:39:25,814] Trial 56 finished with value: 0.5197834902339511 and parameters: {'lr': 0.000977058810565469, 'dropout': 0.2866573732001395, 'n_filters': 120, 'filter_sizes': [4, 5, 6], 'embedding_dim': 300, 'weight_decay': 0.004649197425363979, 'batch_size': 128, 'optimizer_type': 'RMSProp', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:40:43,122] Trial 57 finished with value: 0.5098996104465591 and parameters: {'lr': 0.0006742772169482546, 'dropout': 0.38122613304572, 'n_filters': 80, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0013185802394245104, 'batch_size': 16, 'optimizer_type': 'Adam', 'activation_function': 'ReLU', 'pooling_strategy': 'max'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:41:39,492] Trial 58 finished with value: 0.4865592126217153 and parameters: {'lr': 0.00031616421301796136, 'dropout': 0.3339519462624729, 'n_filters': 100, 'filter_sizes': [2, 3, 4], 'embedding_dim': 100, 'weight_decay': 0.002208640407272525, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:43:48,528] Trial 59 finished with value: 0.5136007418235143 and parameters: {'lr': 0.00041664440337722353, 'dropout': 0.22830176810104033, 'n_filters': 110, 'filter_sizes': [3, 4, 5], 'embedding_dim': 200, 'weight_decay': 0.000900288700173247, 'batch_size': 64, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-09 11:45:00,244] Trial 60 finished with value: 0.48126839473843575 and parameters: {'lr': 0.00023329379580767378, 'dropout': 0.18325370957764972, 'n_filters': 60, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.00814962766646767, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:46:14,342] Trial 61 finished with value: 0.46948254315389526 and parameters: {'lr': 0.0005305044063785922, 'dropout': 0.39105930125708177, 'n_filters': 90, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0016618229866880727, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:47:23,539] Trial 62 finished with value: 0.49015243392851615 and parameters: {'lr': 0.0005007080902041266, 'dropout': 0.4199244563605796, 'n_filters': 70, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0015770490722694986, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:48:39,468] Trial 63 finished with value: 0.4524218584928248 and parameters: {'lr': 0.0006203644517174406, 'dropout': 0.3149699817024764, 'n_filters': 90, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0033632509571336648, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:50:33,307] Trial 64 finished with value: 0.4693866612182723 and parameters: {'lr': 0.0006100746017044024, 'dropout': 0.26827737780658933, 'n_filters': 100, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.0030450761751480288, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'LeakyReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:51:52,139] Trial 65 finished with value: 0.4375289875186152 and parameters: {'lr': 0.0007810557520755111, 'dropout': 0.3136448460533646, 'n_filters': 80, 'filter_sizes': [2, 3, 4], 'embedding_dim': 200, 'weight_decay': 0.005824957320569215, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:53:01,866] Trial 66 finished with value: 0.5111953624420695 and parameters: {'lr': 0.000820748679258202, 'dropout': 0.3500731855712129, 'n_filters': 90, 'filter_sizes': [2, 3, 4], 'embedding_dim': 100, 'weight_decay': 0.005101954781816202, 'batch_size': 128, 'optimizer_type': 'RMSProp', 'activation_function': 'ReLU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:54:35,759] Trial 67 finished with value: 0.5060466523799632 and parameters: {'lr': 0.0006859974870804459, 'dropout': 0.31033191490944284, 'n_filters': 80, 'filter_sizes': [4, 5, 6], 'embedding_dim': 200, 'weight_decay': 0.006372465352598215, 'batch_size': 128, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'max'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:56:26,147] Trial 68 finished with value: 0.633812838130527 and parameters: {'lr': 0.0008803811299919193, 'dropout': 0.33892504235634435, 'n_filters': 100, 'filter_sizes': [3, 4, 5], 'embedding_dim': 200, 'weight_decay': 0.013269863233129488, 'batch_size': 16, 'optimizer_type': 'SGD', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 11:58:31,179] Trial 69 finished with value: 0.43983592506912017 and parameters: {'lr': 0.0009985217684801615, 'dropout': 0.3129550266088358, 'n_filters': 90, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.002448590236253019, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 12:01:16,361] Trial 70 finished with value: 0.4616403844621446 and parameters: {'lr': 0.000990462276469075, 'dropout': 0.30383771250541297, 'n_filters': 130, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.009409550165984703, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 35 with value: 0.4274591205434667.\n",
      "[I 2023-11-09 12:04:00,759] Trial 71 finished with value: 0.4168363617112239 and parameters: {'lr': 0.0007809671359222956, 'dropout': 0.30216765316720884, 'n_filters': 130, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.003736652093157015, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:06:47,465] Trial 72 finished with value: 0.4638541779584355 and parameters: {'lr': 0.0009512519455795678, 'dropout': 0.3017011138600525, 'n_filters': 130, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.010406161020512643, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:09:41,398] Trial 73 finished with value: 0.47017545128862065 and parameters: {'lr': 0.0007598795138692091, 'dropout': 0.2613484479169868, 'n_filters': 140, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.003826864613874032, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:12:29,537] Trial 74 finished with value: 0.46272052079439163 and parameters: {'lr': 0.0008433679017913674, 'dropout': 0.3207887170966327, 'n_filters': 120, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.007598340345636996, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:15:18,860] Trial 75 finished with value: 0.5348352019985517 and parameters: {'lr': 0.0006375374537018924, 'dropout': 0.28770786946732746, 'n_filters': 130, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.0025897477560286705, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:18:11,754] Trial 76 finished with value: 0.4729948801298936 and parameters: {'lr': 0.0009684845225796807, 'dropout': 0.2338704986440295, 'n_filters': 130, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.005966118886076344, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:20:50,716] Trial 77 finished with value: 0.45390866531266105 and parameters: {'lr': 0.0007477249922544359, 'dropout': 0.2763061584536284, 'n_filters': 140, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.003517797183227074, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:23:43,987] Trial 78 finished with value: 0.5031977759467231 and parameters: {'lr': 0.0007577913159492551, 'dropout': 0.2580290685820423, 'n_filters': 140, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.0032304534250660773, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:27:10,394] Trial 79 finished with value: 0.49436474550101495 and parameters: {'lr': 0.0006228114666607493, 'dropout': 0.2817778252685563, 'n_filters': 150, 'filter_sizes': [3, 4, 5], 'embedding_dim': 300, 'weight_decay': 0.004150441830726729, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-09 12:31:13,716] Trial 80 finished with value: 0.556060796810521 and parameters: {'lr': 0.0006573890120055478, 'dropout': 0.24407069570510326, 'n_filters': 120, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.0024868335705130486, 'batch_size': 32, 'optimizer_type': 'RMSProp', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:33:59,036] Trial 81 finished with value: 0.46203165128827095 and parameters: {'lr': 0.0008507936062455343, 'dropout': 0.30071679954065467, 'n_filters': 130, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.005261700976500337, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:36:50,774] Trial 82 finished with value: 0.4709513398508231 and parameters: {'lr': 0.0007822174891742368, 'dropout': 0.2759241682580842, 'n_filters': 140, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.008162462010994935, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:39:45,749] Trial 83 finished with value: 0.4539637722902828 and parameters: {'lr': 0.0005412346541941575, 'dropout': 0.321718241428183, 'n_filters': 140, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.0033736796766276962, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:42:38,569] Trial 84 finished with value: 0.47430073614749646 and parameters: {'lr': 0.0005559101232789663, 'dropout': 0.3400640558188441, 'n_filters': 140, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.002190181750010713, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:45:19,979] Trial 85 finished with value: 0.45077091703812283 and parameters: {'lr': 0.0006972928608623695, 'dropout': 0.3220436753211196, 'n_filters': 140, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.0034337907791627538, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:49:01,941] Trial 86 finished with value: 0.46360771420101327 and parameters: {'lr': 0.000706644592098544, 'dropout': 0.360486139173151, 'n_filters': 150, 'filter_sizes': [4, 5, 6], 'embedding_dim': 300, 'weight_decay': 0.003719712191027524, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'max'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:50:59,181] Trial 87 finished with value: 0.6240688463052114 and parameters: {'lr': 0.0004547384278580583, 'dropout': 0.270925919899711, 'n_filters': 80, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.005147102919629038, 'batch_size': 32, 'optimizer_type': 'SGD', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:53:28,740] Trial 88 finished with value: 0.4862292893230915 and parameters: {'lr': 0.0008733641690026276, 'dropout': 0.23834566157027914, 'n_filters': 110, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.0028011108537361333, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:54:40,071] Trial 89 finished with value: 0.5229785756932365 and parameters: {'lr': 0.0006143225639268834, 'dropout': 0.31672847743837407, 'n_filters': 150, 'filter_sizes': [2, 3, 4], 'embedding_dim': 100, 'weight_decay': 0.0021024669247316995, 'batch_size': 64, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 12:57:43,451] Trial 90 finished with value: 0.5010960474610329 and parameters: {'lr': 0.0007400015604207438, 'dropout': 0.20470040433803763, 'n_filters': 90, 'filter_sizes': [4, 5, 6], 'embedding_dim': 300, 'weight_decay': 0.006090407764255569, 'batch_size': 16, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 13:00:13,854] Trial 91 finished with value: 0.48701603131161797 and parameters: {'lr': 0.0005193673590569993, 'dropout': 0.33094704237964984, 'n_filters': 140, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.0037615943245149865, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 13:02:46,842] Trial 92 finished with value: 0.4778621921108829 and parameters: {'lr': 0.0005878363609405583, 'dropout': 0.2938667135773258, 'n_filters': 140, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.0031354218054418636, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 13:05:21,884] Trial 93 finished with value: 0.45332902669906616 and parameters: {'lr': 0.0006796427697498779, 'dropout': 0.2577423796040169, 'n_filters': 140, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.004106294883242031, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 13:07:55,879] Trial 94 finished with value: 0.4428362672527631 and parameters: {'lr': 0.0006902853208234989, 'dropout': 0.251397610074907, 'n_filters': 130, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.004767628396007372, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 13:10:22,461] Trial 95 finished with value: 0.4497537635680702 and parameters: {'lr': 0.0008704429772769545, 'dropout': 0.2554184266736151, 'n_filters': 120, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.004508211596032044, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 13:12:41,933] Trial 96 finished with value: 0.46511226975255543 and parameters: {'lr': 0.0008445181962253825, 'dropout': 0.2516209775191645, 'n_filters': 120, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.007290330017388056, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 13:15:15,201] Trial 97 finished with value: 0.4621409343348609 and parameters: {'lr': 0.0009177188124425804, 'dropout': 0.20438909598704036, 'n_filters': 120, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.004823821039145876, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ReLU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 13:17:44,323] Trial 98 finished with value: 0.43953700984517735 and parameters: {'lr': 0.00048142855354538515, 'dropout': 0.2338286624363612, 'n_filters': 110, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.006783161692245204, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}. Best is trial 71 with value: 0.4168363617112239.\n",
      "[I 2023-11-09 13:18:42,046] Trial 99 finished with value: 0.5726829469203949 and parameters: {'lr': 0.0004935610940995296, 'dropout': 0.22605940574527408, 'n_filters': 120, 'filter_sizes': [2, 3, 4], 'embedding_dim': 100, 'weight_decay': 0.006489777661179597, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'max'}. Best is trial 71 with value: 0.4168363617112239.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'lr': 0.0007809671359222956, 'dropout': 0.30216765316720884, 'n_filters': 130, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.003736652093157015, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}\n",
      "Validation Loss with best hyperparameters: 0.4168363617112239\n"
     ]
    }
   ],
   "source": [
    "### Import optuna to test through best hyperparameter configuration:\n",
    "## Applied only to the baseline sadness and joy dataset with no hyperparameter configuration:\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # 1. Define range of hyperparameters:\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.7)\n",
    "    n_filters = trial.suggest_int('n_filters', 50, 150, 10)\n",
    "    filter_sizes = trial.suggest_categorical('filter_sizes', [[2,3,4], [3,4,5], [4,5,6]])\n",
    "    emb_dim = trial.suggest_categorical('embedding_dim', [100, 200, 300])\n",
    "    \n",
    "    # Regularization\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-1, log=True)\n",
    "    \n",
    "    # Training specifics\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    optimizer_type = trial.suggest_categorical('optimizer_type', ['Adam', 'SGD', 'RMSProp'])\n",
    "    \n",
    "    # Model specifics\n",
    "    activation_function = trial.suggest_categorical('activation_function', ['ReLU', 'LeakyReLU', 'ELU'])\n",
    "    pooling_strategy = trial.suggest_categorical('pooling_strategy', ['max', 'avg'])\n",
    "    \n",
    "    # 2. Create and train model with these hyperparameters:\n",
    "    model = TextCNN(vocab_size, emb_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx).to(device)\n",
    "    # Initialize the appropriate optimizer\n",
    "    if optimizer_type == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_type == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_type == 'RMSProp':\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer type\")\n",
    "\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    \n",
    "    best_valid_loss = float('inf')\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        valid_loss = evaluate(model, val_loader, criterion, device)\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            \n",
    "    # 3. Return validation loss for this set of hyperparameters:\n",
    "    return best_valid_loss\n",
    "\n",
    "# Use Optuna to find best hyperparameters:\n",
    "study = optuna.create_study(direction='minimize')  # We want to minimize the validation loss\n",
    "study.optimize(objective, n_trials=100)  # Number of trials can be adjusted based on computational resources\n",
    "\n",
    "# Get the best hyperparameters:\n",
    "best_params = study.best_params\n",
    "best_loss = study.best_value\n",
    "print(f\"Best hyperparameters:\\n{best_params}\\nValidation Loss with best hyperparameters: {best_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a0d11",
   "metadata": {},
   "source": [
    "<b>Best combination from using optuna trials after performing 100 trials: Number 71 is the optimal configuration:\n",
    "Best hyperparameters:\n",
    "{'lr': 0.0007809671359222956, 'dropout': 0.30216765316720884, 'n_filters': 130, 'filter_sizes': [2, 3, 4], 'embedding_dim': 300, 'weight_decay': 0.003736652093157015, 'batch_size': 32, 'optimizer_type': 'Adam', 'activation_function': 'ELU', 'pooling_strategy': 'avg'}\n",
    "Validation Loss with best hyperparameters: 0.4168363617112239 <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b42df102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Main Task 3 - Alternative dataset\n",
    "## This time with sadness and optimism:\n",
    "#Import the main tools for the task:\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74e83aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label_text  label_number\n",
      "0  â€œWorry is a down payment on a problem you may ...        joy             2\n",
      "1  My roommate: it's okay that we can't spell bec...    sadness             0\n",
      "2  No but that's so cute. Atsu was probably shy a...   optimism             1\n",
      "3  Rooneys fucking untouchable isn't he? Been fuc...    sadness             0\n",
      "4  it's pretty depressing when u hit pan on ur fa...      anger             3\n",
      "Length of train_data: 3257\n",
      "Length of validation_data: 374\n",
      "Length of test_data: 1421\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary of the mapping between numbers and labels\n",
    "mappings = {\"anger\": 3, \"joy\": 2, \"optimism\": 1, \"sadness\": 0}\n",
    "\n",
    "def load_data(mapping_dictionary:dict , tweet_file_path: str, label_file_path:str)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    function to load both the tweets and the labels, combine them together as pandas dataframe\n",
    "    \"\"\"\n",
    "    with open(tweet_file_path, 'r', encoding=\"utf-8\") as file:\n",
    "        tweets = [line.strip() for line in file.readlines()]\n",
    "\n",
    "    with open(label_file_path, 'r', encoding=\"utf-8\") as file:\n",
    "        label_numbers = [int(line.strip()) for line in file.readlines()]\n",
    "\n",
    "    label_texts = [next((key for key, value in mapping_dictionary.items() if value == label_number), None) for label_number in label_numbers]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'text': tweets,\n",
    "        'label_text': label_texts,\n",
    "        'label_number': label_numbers\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# make sure the files in the same directory as the notebook\n",
    "train_data = load_data(mappings, \"train_text.txt\", \"train_labels.txt\")\n",
    "validation_data = load_data(mappings, \"val_text.txt\", \"val_labels.txt\")\n",
    "test_data = load_data(mappings, \"test_text.txt\", \"test_labels.txt\")\n",
    "\n",
    "# print the head of one of them\n",
    "print(train_data.head())\n",
    "\n",
    "# Print the length of each data\n",
    "print(f\"Length of train_data: {len(train_data)}\")\n",
    "print(f\"Length of validation_data: {len(validation_data)}\")\n",
    "print(f\"Length of test_data: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6b354e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label_text  label_number\n",
      "1  My roommate: it's okay that we can't spell bec...    sadness             0\n",
      "2  No but that's so cute. Atsu was probably shy a...   optimism             1\n",
      "3  Rooneys fucking untouchable isn't he? Been fuc...    sadness             0\n",
      "5  @user but your pussy was weak from what I hear...    sadness             0\n",
      "7  Tiller and breezy should do a collab album. Ra...   optimism             1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "2108\n"
     ]
    }
   ],
   "source": [
    "### Replace the emotion of interest\n",
    "# First, we grab the main dataset again, complete with training and validation packs:\n",
    "## Now we filter and create the subset dataset with the replaced emotion:\n",
    "# First one will be sadness (0) and optimism (1)\n",
    "sadoptlist = ['sadness','optimism']\n",
    "train_data = train_data[train_data['label_text'].isin(sadoptlist)]\n",
    "\n",
    "#Alright, got only the sadness and joy!\n",
    "print(train_data.head())\n",
    "print(type(train_data))\n",
    "## Count total with only sadness and joy:\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf312413",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we do the same for the other 2 sets for validation and testing:\n",
    "validation_data = validation_data[validation_data['label_text'].isin(sadoptlist)]\n",
    "test_data = test_data[test_data['label_text'].isin(sadoptlist)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0146f68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label_text  label_number\n",
      "0  @user @user Oh, hidden revenge and anger...I r...    sadness             0\n",
      "1  if not then #teamchristine bc all tana has don...    sadness             0\n",
      "2  Hey @user #Fields in #skibbereen give your onl...    sadness             0\n",
      "3  Why have #Emmerdale had to rob #robron of havi...    sadness             0\n",
      "4  @user I would like to hear a podcast of you go...    sadness             0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "257\n"
     ]
    }
   ],
   "source": [
    "#Validation set got only the sadness and optimism!\n",
    "print(validation_data.head())\n",
    "print(type(validation_data))\n",
    "# Count validation:\n",
    "print(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec60b639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label_text  label_number\n",
      "1  @user Interesting choice of words... Are you c...    sadness             0\n",
      "3  @user Welcome to #MPSVT! We are delighted to h...   optimism             1\n",
      "4                       What makes you feel #joyful?   optimism             1\n",
      "5                                    i am revolting.    sadness             0\n",
      "9  @user Get Donovan out of your soccer booth. He...    sadness             0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "916\n"
     ]
    }
   ],
   "source": [
    "#Test set got only the sadness and joy!\n",
    "print(test_data.head())\n",
    "print(type(test_data))\n",
    "#count:\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "438d3c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize the strings in text column:\n",
    "def tokenize_sentence(sentence: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenizes a sentence using nltk's word_tokenize method.\n",
    "\n",
    "    Args:\n",
    "    - sentence (str): The sentence to tokenize.\n",
    "\n",
    "    Returns:\n",
    "    - List of tokens.\n",
    "    \"\"\"\n",
    "    return word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dba59e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we apply the tokenizer to the new subset and the validation/testing kit:\n",
    "train_data['tokenized_text'] = train_data['text'].apply(tokenize_sentence)\n",
    "validation_data['tokenized_text'] = validation_data['text'].apply(tokenize_sentence)\n",
    "test_data['tokenized_text'] = test_data['text'].apply(tokenize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7d17fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    [My, roommate, :, it, 's, okay, that, we, ca, ...\n",
      "2    [No, but, that, 's, so, cute, ., Atsu, was, pr...\n",
      "3    [Rooneys, fucking, untouchable, is, n't, he, ?...\n",
      "5    [@, user, but, your, pussy, was, weak, from, w...\n",
      "7    [Tiller, and, breezy, should, do, a, collab, a...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# now the data has a new column \"tokenized_text\" which is a list of tokens\n",
    "#print(train_data.head())\n",
    "print(train_data['tokenized_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93645ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Maybe', 'loyal', '`', 'appearance']\n"
     ]
    }
   ],
   "source": [
    "# Build a set of all unique tokens in the training data\n",
    "vocab_set = set()\n",
    "for tokens in train_data['tokenized_text']:\n",
    "    vocab_set.update(tokens)\n",
    "\n",
    "# Convert the set to a list to index tokens\n",
    "vocab_list = list(vocab_set)\n",
    "\n",
    "print(vocab_list[:4])\n",
    "\n",
    "# Create a word to index mapping\n",
    "word_to_index = {word: index for index, word in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8bc4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add OOV token and its index to the vocabulary. This is because some tokens in the val/test might have vocabulary not in training\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "if OOV_TOKEN not in word_to_index:\n",
    "    word_to_index[OOV_TOKEN] = len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19e3d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main function for conversion of tokens from previous step to index numbers:\n",
    "def tokens_to_numbers(tokens: list, word_to_index: dict) -> list:\n",
    "    \"\"\"\n",
    "    Converts a list of tokens to their corresponding indices using a word-to-index mapping.\n",
    "    Returns the index of OOV_TOKEN for out-of-vocabulary words.\n",
    "    \"\"\"\n",
    "    return [word_to_index.get(token, word_to_index[OOV_TOKEN]) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2731c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the function for token to index conversion:\n",
    "train_data['numeric_tokens'] = train_data['tokenized_text'].apply(lambda x: tokens_to_numbers(x, word_to_index))\n",
    "validation_data['numeric_tokens'] = validation_data['tokenized_text'].apply(lambda x: tokens_to_numbers(x, word_to_index))\n",
    "test_data['numeric_tokens'] = test_data['tokenized_text'].apply(lambda x: tokens_to_numbers(x, word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccae41ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1       [860, 3304, 7656, 6035, 5995, 3419, 6988, 2848...\n",
      "2       [4295, 236, 6988, 5995, 7359, 6378, 3328, 4109...\n",
      "3       [7509, 245, 2603, 4774, 2729, 3230, 3124, 6401...\n",
      "5       [4129, 3368, 236, 4350, 1576, 3137, 3387, 2471...\n",
      "7       [6409, 579, 6920, 962, 7497, 2934, 5047, 3347,...\n",
      "                              ...                        \n",
      "3250    [4129, 3368, 4262, 1675, 6323, 7416, 4506, 719...\n",
      "3251    [4129, 3368, 6210, 5103, 4069, 3322, 2770, 312...\n",
      "3254    [4129, 3368, 4129, 3368, 4129, 3368, 4129, 336...\n",
      "3255    [2736, 531, 2934, 7416, 699, 3124, 2983, 5379,...\n",
      "3256    [4129, 3368, 4129, 3368, 7592, 1129, 1172, 349...\n",
      "Name: numeric_tokens, Length: 2108, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Now the df has a new column \"numeric_tokens\"\n",
    "#print(train_data.head())\n",
    "print(train_data['numeric_tokens'])\n",
    "\n",
    "## Seems alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ca1b8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Maximum Length is: 48\n"
     ]
    }
   ],
   "source": [
    "# Figure the maximum sequence length, but since tweets are conveniently short\n",
    "# The maximum length of tweet is defined:\n",
    "MAX_SEQUENCE_LENGTH = max(train_data['numeric_tokens'].apply(len))\n",
    "print('The Maximum Length is:', MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eed171f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padding is required for the dataset:\n",
    "def pad_sequence(numeric_tokens: list, max_length: int) -> list:\n",
    "    \"\"\"\n",
    "    Pads a sequence to a given length. If sequence is shorter than the target length,\n",
    "    it'll be padded with zeros.\n",
    "    \"\"\"\n",
    "    return numeric_tokens + [0]*(max_length - len(numeric_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3d963f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the padding:\n",
    "train_data['padded_tokens'] = train_data['numeric_tokens'].apply(lambda x: pad_sequence(x, MAX_SEQUENCE_LENGTH))\n",
    "validation_data['padded_tokens'] = validation_data['numeric_tokens'].apply(lambda x: pad_sequence(x, MAX_SEQUENCE_LENGTH))\n",
    "test_data['padded_tokens'] = test_data['numeric_tokens'].apply(lambda x: pad_sequence(x, MAX_SEQUENCE_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09aa1996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1       [860, 3304, 7656, 6035, 5995, 3419, 6988, 2848...\n",
      "2       [4295, 236, 6988, 5995, 7359, 6378, 3328, 4109...\n",
      "3       [7509, 245, 2603, 4774, 2729, 3230, 3124, 6401...\n",
      "5       [4129, 3368, 236, 4350, 1576, 3137, 3387, 2471...\n",
      "7       [6409, 579, 6920, 962, 7497, 2934, 5047, 3347,...\n",
      "                              ...                        \n",
      "3250    [4129, 3368, 4262, 1675, 6323, 7416, 4506, 719...\n",
      "3251    [4129, 3368, 6210, 5103, 4069, 3322, 2770, 312...\n",
      "3254    [4129, 3368, 4129, 3368, 4129, 3368, 4129, 336...\n",
      "3255    [2736, 531, 2934, 7416, 699, 3124, 2983, 5379,...\n",
      "3256    [4129, 3368, 4129, 3368, 7592, 1129, 1172, 349...\n",
      "Name: padded_tokens, Length: 2108, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Now the df has a new column \"padded_tokens\". \n",
    "# Usually we shouldn't load all that to memory but once again, Tweets are small, so it is ok:\n",
    "#print(train_data.head())\n",
    "print(train_data['padded_tokens'])\n",
    "\n",
    "## Seems the same as previous..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb5669d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to list:\n",
    "train_dataset = TextDataset(train_data['padded_tokens'].tolist(), train_data['label_number'].tolist())\n",
    "val_dataset = TextDataset(validation_data['padded_tokens'].tolist(), validation_data['label_number'].tolist())\n",
    "test_dataset = TextDataset(test_data['padded_tokens'].tolist(), test_data['label_number'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fef5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Batch size to work with: GS of 32, 64 and 128:\n",
    "batch_size = 32\n",
    "\n",
    "#Loaded and ready:\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e613dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7749\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Main hyperparameters: Using the \"best\" configuration from sadjoy set:\n",
    "vocab_size = len(word_to_index) + 1  # +1 for padding \n",
    "print(vocab_size)\n",
    "embedding_dim = 300  \n",
    "n_filters = 110\n",
    "filter_sizes = [2,3,4]\n",
    "#output_dim = len(train_data['label_number'].unique())\n",
    "output_dim = 1\n",
    "print(output_dim)\n",
    "dropout=0.1\n",
    "#dropout=0.24691844248854944\n",
    "pad_idx = 0  # Assuming 0 is the index for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "832be234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's embedding layer size: torch.Size([7749, 300])\n"
     ]
    }
   ],
   "source": [
    "## Run the main CNN model builder:\n",
    "model_sadopt = TextCNN(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx).to(device)\n",
    "print(\"Model's embedding layer size:\", model_sadopt.embedding.weight.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31be1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.embedding.weight.data.copy_(torch.tensor(embedding_matrix))\n",
    "model_sadopt.embedding.weight.requires_grad = False  # Freeze the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79ed9df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "## BCE loss is used since it's only two categories\n",
    "criterion = nn.BCELoss().to(device)\n",
    "#criterion = nn.CrossEntropyLoss().to(device)\n",
    "#lr=0.000682446937962706\n",
    "optimizer = torch.optim.Adam(model_sadopt.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b0a5067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.298\n",
      "\t Val. Loss: 1.684\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.266\n",
      "\t Val. Loss: 3.905\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.337\n",
      "\t Val. Loss: 3.905\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.314\n",
      "\t Val. Loss: 4.507\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.485\n",
      "\t Val. Loss: 3.888\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.770\n",
      "\t Val. Loss: 3.969\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.689\n",
      "\t Val. Loss: 7.267\n",
      "Epoch: 08\n",
      "\tTrain Loss: 1.057\n",
      "\t Val. Loss: 8.875\n",
      "Epoch: 09\n",
      "\tTrain Loss: 1.196\n",
      "\t Val. Loss: 27.482\n",
      "Epoch: 10\n",
      "\tTrain Loss: 1.944\n",
      "\t Val. Loss: 9.102\n",
      "Epoch: 11\n",
      "\tTrain Loss: 1.163\n",
      "\t Val. Loss: 12.940\n",
      "Epoch: 12\n",
      "\tTrain Loss: 1.122\n",
      "\t Val. Loss: 11.982\n",
      "Epoch: 13\n",
      "\tTrain Loss: 1.376\n",
      "\t Val. Loss: 22.436\n",
      "Epoch: 14\n",
      "\tTrain Loss: 2.456\n",
      "\t Val. Loss: 21.033\n",
      "Epoch: 15\n",
      "\tTrain Loss: 2.408\n",
      "\t Val. Loss: 28.721\n",
      "Epoch: 16\n",
      "\tTrain Loss: 2.243\n",
      "\t Val. Loss: 17.180\n",
      "Epoch: 17\n",
      "\tTrain Loss: 3.168\n",
      "\t Val. Loss: 18.607\n",
      "Epoch: 18\n",
      "\tTrain Loss: 2.116\n",
      "\t Val. Loss: 16.897\n",
      "Epoch: 19\n",
      "\tTrain Loss: 4.014\n",
      "\t Val. Loss: 16.611\n",
      "Epoch: 20\n",
      "\tTrain Loss: 2.498\n",
      "\t Val. Loss: 20.074\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs at 20\n",
    "##Now we are performing the main training for sadopt\n",
    "n_epochs = 20\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model_sadopt, train_loader, optimizer, criterion, device)\n",
    "    valid_loss = evaluate(model_sadopt, val_loader, criterion, device)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_sadopt.state_dict(), 'sadopt_model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a623402f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save the model:\n",
    "torch.save(model_sadopt, 'sadoptmodel.pth')\n",
    "## Load the base initial model for sadjoy:\n",
    "model_sadopt.load_state_dict(torch.load('sadopt_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "678c04fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All predictions, labels and testing loss values\n",
    "all_preds, all_labels, test_loss = evaluate_model(model_sadopt, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bed2b411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.684\n",
      "F1 Score: 0.678\n"
     ]
    }
   ],
   "source": [
    "#### Now we calculate accuracy and F1 score for the base version of Sadopt model:\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average='macro') # 'macro' calculates metrics for each label and then does an unweighted mean\n",
    "\n",
    "#precision = precision_score(all_labels, all_preds, average='macro')\n",
    "#recall = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79836dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now we use the hyperparameters from sadjoy ###\n",
    "## Config extracted with optima are used here for sad-optimism:\n",
    "## Best Batch size = 32\n",
    "batch_size = 32\n",
    "\n",
    "#Loaded and ready:\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ab721e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7749\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Main hyperparameters: Using the \"optima best\" configuration from sadjoy set:\n",
    "vocab_size = len(word_to_index) + 1  # +1 for padding \n",
    "print(vocab_size)\n",
    "embedding_dim = 300\n",
    "#Filters increased to 130:\n",
    "n_filters = 130\n",
    "filter_sizes = [2,3,4]\n",
    "#output_dim = len(train_data['label_number'].unique())\n",
    "output_dim = 1\n",
    "print(output_dim)\n",
    "#Best dropout is: 0.30216765316720884\n",
    "dropout=0.30216765316720884\n",
    "pad_idx = 0  # Assuming 0 is the index for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e938d067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's embedding layer size: torch.Size([7749, 300])\n"
     ]
    }
   ],
   "source": [
    "## Run the main CNN model builder:\n",
    "model_sadopt_optima = TextCNN(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx).to(device)\n",
    "print(\"Model's embedding layer size:\", model_sadopt_optima.embedding.weight.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9d8f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.embedding.weight.data.copy_(torch.tensor(embedding_matrix))\n",
    "model_sadopt_optima.embedding.weight.requires_grad = False  # Freeze the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f86d65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "## BCE loss is used since it's only two categories\n",
    "criterion = nn.BCELoss().to(device)\n",
    "#criterion = nn.CrossEntropyLoss().to(device)\n",
    "#Learning rate changed to 0.0007809671359222956 as per optima best trial from sadjoy\n",
    "optimizer = torch.optim.Adam(model_sadopt_optima.parameters(), lr=0.0007809671359222956)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6255b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.590\n",
      "\t Val. Loss: 0.598\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.329\n",
      "\t Val. Loss: 0.536\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.182\n",
      "\t Val. Loss: 0.543\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.098\n",
      "\t Val. Loss: 0.554\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.062\n",
      "\t Val. Loss: 0.544\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.038\n",
      "\t Val. Loss: 0.559\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.033\n",
      "\t Val. Loss: 0.608\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.025\n",
      "\t Val. Loss: 0.583\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.025\n",
      "\t Val. Loss: 0.599\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.026\n",
      "\t Val. Loss: 0.606\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.025\n",
      "\t Val. Loss: 0.622\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.024\n",
      "\t Val. Loss: 0.667\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.018\n",
      "\t Val. Loss: 0.606\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.024\n",
      "\t Val. Loss: 0.610\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.011\n",
      "\t Val. Loss: 0.657\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.008\n",
      "\t Val. Loss: 0.702\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.014\n",
      "\t Val. Loss: 0.682\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.010\n",
      "\t Val. Loss: 0.660\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.014\n",
      "\t Val. Loss: 0.697\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.010\n",
      "\t Val. Loss: 0.658\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs at 20\n",
    "##Now we are performing the main training for sadopt\n",
    "n_epochs = 20\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model_sadopt_optima, train_loader, optimizer, criterion, device)\n",
    "    valid_loss = evaluate(model_sadopt_optima, val_loader, criterion, device)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_sadopt_optima.state_dict(), 'sadopt_model_optima.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6bc84cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save the model:\n",
    "torch.save(model_sadopt_optima, 'sadoptmodeloptima.pth')\n",
    "## Load the base initial model for sadjoy:\n",
    "model_sadopt_optima.load_state_dict(torch.load('sadopt_model_optima.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "71fb5c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### All predictions, labels and testing loss values\n",
    "all_preds, all_labels, test_loss = evaluate_model(model_sadopt_optima, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1771c2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.733\n",
      "F1 Score: 0.687\n"
     ]
    }
   ],
   "source": [
    "#### Now we calculate accuracy and F1 score for the final Optima-configuration model:\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average='macro') # 'macro' calculates metrics for each label and then does an unweighted mean\n",
    "\n",
    "#precision = precision_score(all_labels, all_preds, average='macro')\n",
    "#recall = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3643e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---- Finished ---- ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
